\documentclass[12pt,a4paper]{report}
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL /tmp/awYD5l7zFZ/latexdiff-vc-/main.tex   Tue Sep 22 14:14:44 2020
%DIF ADD main.tex                                 Tue Sep 22 17:10:00 2020
\usepackage[english]{babel}
\usepackage{newlfont}
\usepackage{color}
\usepackage{graphicx} % figures...
\usepackage{amsmath} % math symb
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % norm sign
\usepackage{enumerate}
\usepackage{url}
\usepackage{gensymb} % Â° symbol
\usepackage{bm} % bold math symbols
\usepackage{amssymb} % calligraphy
\usepackage{soul} % highlighting text
\usepackage[labelfont={bf,it},textfont=it, margin=1cm]{caption} % caption settings
\usepackage{subcaption} % subfigures
\usepackage{natbib} % biblio
\bibliographystyle{plain}
\usepackage[nottoc]{tocbibind} % show bib in toc
\usepackage{hyperref} % show intro in toc
\usepackage{tocloft} % toc tewaking

%DIF 22a22-24
% Command for creating .diff file %DIF > 
% latexdiff-vc --git -r --pdf --flatten main.tex %DIF > 
 %DIF > 
%DIF -------
\textwidth=450pt
\oddsidemargin=0pt
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
\newcommand{\DIFscaledelfig}{0.5}
%DIF HIGHLIGHTGRAPHICS PREAMBLE %DIF PREAMBLE
\RequirePackage{settobox} %DIF PREAMBLE
\RequirePackage{letltxmacro} %DIF PREAMBLE
\newsavebox{\DIFdelgraphicsbox} %DIF PREAMBLE
\newlength{\DIFdelgraphicswidth} %DIF PREAMBLE
\newlength{\DIFdelgraphicsheight} %DIF PREAMBLE
% store original definition of \includegraphics %DIF PREAMBLE
\LetLtxMacro{\DIFOincludegraphics}{\includegraphics} %DIF PREAMBLE
\newcommand{\DIFaddincludegraphics}[2][]{{\color{blue}\fbox{\DIFOincludegraphics[#1]{#2}}}} %DIF PREAMBLE
\newcommand{\DIFdelincludegraphics}[2][]{% %DIF PREAMBLE
\sbox{\DIFdelgraphicsbox}{\DIFOincludegraphics[#1]{#2}}% %DIF PREAMBLE
\settoboxwidth{\DIFdelgraphicswidth}{\DIFdelgraphicsbox} %DIF PREAMBLE
\settoboxtotalheight{\DIFdelgraphicsheight}{\DIFdelgraphicsbox} %DIF PREAMBLE
\scalebox{\DIFscaledelfig}{% %DIF PREAMBLE
\parbox[b]{\DIFdelgraphicswidth}{\usebox{\DIFdelgraphicsbox}\\[-\baselineskip] \rule{\DIFdelgraphicswidth}{0em}}\llap{\resizebox{\DIFdelgraphicswidth}{\DIFdelgraphicsheight}{% %DIF PREAMBLE
\setlength{\unitlength}{\DIFdelgraphicswidth}% %DIF PREAMBLE
\begin{picture}(1,1)% %DIF PREAMBLE
\thicklines\linethickness{2pt} %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\framebox(1,1){}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\line( 1,1){1}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,1){\line(1,-1){1}}}% %DIF PREAMBLE
\end{picture}% %DIF PREAMBLE
}\hspace*{3pt}}} %DIF PREAMBLE
} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbegin}{\DIFaddbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddend}{\DIFaddend} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbegin}{\DIFdelbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelend}{\DIFdelend} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbegin}{\DIFOaddbegin \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbegin}{\DIFOdelbegin \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbeginFL}{\DIFaddbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddendFL}{\DIFaddendFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbeginFL}{\DIFdelbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelendFL}{\DIFdelendFL} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbeginFL}{\DIFOaddbeginFL \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbeginFL}{\DIFOdelbeginFL \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}
    \begin{titlepage}
    \begin{center}
        {{\Large{\textsc{Alma Mater Studiorum $\cdot$ University of  Bologna}}}}
        \rule[0.1cm]{15.8cm}{0.1mm}
        \rule[0.5cm]{15.8cm}{0.6mm}
        \\\vspace{3mm}
        {\small{\bf School of Science \\
        Department of Physics and Astronomy\\
        Master Degree in Physics}}
    \end{center}

    \vspace{23mm}

    \begin{center}
        \LARGE{\bf Dataset Generation for the Training of Neural Networks Oriented toward Histological Image Segmentation}\\
    \end{center}

    \vspace{40mm} \par \noindent

    \begin{minipage}[t]{0.47\textwidth}
        {\large{\bf Supervisor: \vspace{2mm}\\
        Dr. Enrico Giampieri\\\\
        \bf Co-supervisor: \vspace{2mm}\\
        Dr. Nico Curti\\\\}}
    \end{minipage}

    %
    \hfill
    %
    \begin{minipage}[t]{0.47\textwidth}\raggedleft
        \textcolor{black}{
            {\large{\bf Submitted by:
                \vspace{2mm}\\
                {Alessandro d'Agostino}}}
        }
    \end{minipage}

    \vspace{21mm}

    \begin{center}
        Academic Year 2019/2020
    \end{center}
\end{titlepage}

    \newpage \ \newpage

    \chapter*{Abstract}
Abstract.....

    \clearpage

    % tweaking toc
    \renewcommand{\contentsname}{Table of Contents}
    \renewcommand\cftchapafterpnum{\par\addvspace{6pt}}
    \renewcommand\cftsecafterpnum{\par\addvspace{4pt}}
    \tableofcontents{}
    \newpage \ \newpage

    \chapter*{Introduction}
\label{chap:intro}
\addcontentsline{toc}{chapter}{\nameref{chap:intro}} %manually adding the unnumbered chapter to toc
In the last decades, the development of Machine Learning (ML) and Deep Learning (DL) techniques has contaminated every aspect of the scientific world, with interesting results in many different research fields. The biomedical field is no exception to this and a lot of promising applications are taking form, especially as Computer-Aided Detection (CAD) systems which are tools for the support for physicians during the diagnostic process. Medical doctors and the healthcare system in general collect a huge amount of data from patients during all the treatment, screening, and analysis activities in many different shapes, from anagraphical data to blood analysis to clinical images.

In fact in medicine, the study of images is ubiquitous and countless diagnostic procedures rely on it, such as X-ray imaging (CAT), nuclear imaging (SPECT, PET), Magnetic resonance, and visual inspection of histological specimens after biopsies. The branch of artificial intelligence in the biomedical field that handles image analysis to assist physicians in their clinical decisions goes under the name of Digital Pathology Image Analysis (DPIA).
In this thesis work, I want to focus on some of the beneficial aspects introduced by DPIA in the histological images analysis and some particular issues in the development of DL models able to handle this kind of procedure.

Nowadays the great majority of analysis of histological specimens occurs through visual inspection, carried out by highly qualified experts. Some analysis, as cancer detection, requires the ability to distinguish if a region of tissue is healthy or not with high precision in very wide specimens. This kind of procedure is typically very complex and requires prolonged times of analysis besides substantial economic efforts. Furthermore, the designated personnel for this type of analysis is often limited, leading to delicate issues of priority assignment while scheduling analysis, based on the estimated patient's clinical development. Some sort of support to this analysis procedure is therefore necessary.

The problem of recognizing regions with different features within an image and detect their borders is known in computer vision as the segmentation task, and it's quite widespread with countless different applications, allowing a sort of automatic image interpretation. In ML the segmentation problem is usually faced as a supervised task, hence the algorithm in order to be trained properly requires an appropriate quantity of pre-labeled images, from which learn the rules through which distinguish different regions. This means that the development of segmentation algorithms for a specific application, as would be the one on histological images, would require a lot of starting material, previously analyzed from the same qualified expert encharged of the visual inspection mentioned before. A human operator thus is required to manually track the boundaries, for example, between healthy and tumoral regions within a sample of tissue and to label them with their identity, as in Figure \ref{fig:man_seg}. The more the algorithm to train is complex the more starting material is required to adjust the model's parameters and reach the desired efficacy.

\begin{figure}
    \centering
    \includegraphics[width = 0.7\textwidth]{images/manual_seg}
    \caption{Interleaving of tumor (green annotation) and non-tumor (yellow annotation) regions \cite{Ki67}.}
    \label{fig:man_seg}
\end{figure}

The latest developed segmentation algorithms are based on DL techniques, hence based on the implementation of intricated Neural Networks (NN) which process the input images and produce the corresponding segmentation. Those models are typically very complex, with millions of parameters to adjust and tune, therefore they need a huge amount of pre-labeled images to learn their segmentation rules. This need for data is exactly the main focus of my thesis work. The shortage of ground truth images is indeed one of the toughest hurdles to overcome during the development of DL-based algorithms. Another important aspect to bear in mind is the quality of the ground truth material. It's impossible for humans to label boundaries of different regions with pixel-perfect precision, while for machines the more precise is the input the more effective is the resulting algorithm.

Different approaches have already been explored to overcome this problem, and they are mainly based on the generation of synthetic labeled data to use during the training phase. Some techniques achieve data augmentation manipulating already available images and then generating \textit{new} images, but as we will see \DIFdelbegin \DIFdel{later }\DIFdelend this approach suffers from different issues. \DIFdelbegin \DIFdel{The technique that }\DIFdelend \DIFaddbegin \DIFadd{Here, I want to make an overview of some other interesting works on the generation of synthetic histological images, which have followed completely different paths and strategies from mine.
}

\DIFadd{The first work I want to cite is a work from Ben Cheikh }\textit{\DIFadd{et al.}} \DIFadd{from 2017 \mbox{%DIFAUXCMD
\cite{10.1117/12.2254452}}\hspace{0pt}%DIFAUXCMD
. In this work, they present a methodology for the generation of synthetic images of different types of breast carcinomas. They propose a method completely based on two-dimensional morphology operation, as successive image dilations and erosions. With the modulation of a very restricted number of parameters, regulating the abundance of the objects, their distribution in the image, and their shapes they are able to reconstruct realistic images reflecting different histological situations. In Figure \ref{fig:morpho_tripl} is shown an example of generated material besides a real histological H\&E stained sample. Starting from a generated segmentation mask which defines the }\textit{\DIFadd{tumoral pattern}} \DIFadd{the production of the synthetic image passes through successive steps, as the generation of characteristic collagen fibers around the structure, the injection of all the immune system cells, and some general
final refinements.
}

    \begin{figure}[ht]
        \centering
        \begin{subfigure}[t]{0.3\textwidth}
             \centering
             \includegraphics[width = \textwidth]{images/morpho_mask}
             \caption{}
             \label{fig:morpho_mask}
        \end{subfigure}
        \quad
        \begin{subfigure}[t]{0.3\textwidth}
             \centering
             \includegraphics[width = \textwidth]{images/morpho_model}
             \caption{}
             \label{fig:morpho_model}
        \end{subfigure}
        \quad
        \begin{subfigure}[t]{0.3\textwidth}
             \centering
             \includegraphics[width = \textwidth]{images/morpho_real}
             \caption{}
             \label{fig:morpho_real}
        \end{subfigure}
        \caption{\DIFaddFL{Example of generated tumoral pattern (left), which acts as segmentation mask, of generated image (center) and a real example of the tissue to recreate, from \mbox{%DIFAUXCMD
\cite{10.1117/12.2254452}}\hspace{0pt}%DIFAUXCMD
.}}
        \label{fig:morpho_tripl}
    \end{figure}

\DIFadd{The second work I want to mention is based on a DL-base technique, which approaches synthetic image generation using a specific cGAN architecture inspired to the ``U-net'' \mbox{%DIFAUXCMD
\cite{Senaras2018} }\hspace{0pt}%DIFAUXCMD
model, as will be described in Figure \ref{fig:unet} in section \ref{ssec:soa_seg}. This model works with Ki67 stained samples of breast cancer tissue, and it is able to generate high-fidelity images starting from a given segmentation mask. Those starting segmentation masks tough are obtained through the processing of other real histological samples, via a nuclei-detection algorithm. The differences between real and synthetic samples are imperceptible, and the material generated in this work has effectively fooled experts, who qualified it has indistinguishable from the real one. In Figure \ref{fig:cgan_tripl} an example of a real image, a generated one, and their corresponding segmentation mask.
}

    \begin{figure}[t]
        \centering
        \begin{subfigure}[t]{0.3\textwidth}
             \centering
             \includegraphics[width = \textwidth]{images/cgan_mask}
             \caption{}
             \label{fig:cgan_mask}
        \end{subfigure}
        \quad
        \begin{subfigure}[t]{0.3\textwidth}
             \centering
             \includegraphics[width = \textwidth]{images/cgan_model}
             \caption{}
             \label{fig:cgan_model}
        \end{subfigure}
        \quad
        \begin{subfigure}[t]{0.3\textwidth}
             \centering
             \includegraphics[width = \textwidth]{images/cgan_real}
             \caption{}
             \label{fig:cgan_real}
        \end{subfigure}
        \caption{\DIFaddFL{Example of generated tumoral pattern (left), which acts as segmentation mask, of generated image (center) and a real example of the tissue to recreate, from \mbox{%DIFAUXCMD
\cite{Senaras2018}}\hspace{0pt}%DIFAUXCMD
.}}
        \label{fig:cgan_tripl}
    \end{figure}


\DIFadd{Both of the two before-mentioned strategies produces realistic (or even perfect) results, but they are based onto considerations and analysis limited only to the aspect of the images. In the first work, the segmentation mask is produced in an almost full-random way, while in the second the segmentation mask is extracted starting from an actual real histological samples. The target of the present work instead lies in between those two approaches, and wants to produce randomized new images following a plausible modelization based on physical and histological considerations.
}

\DIFadd{The technique }\DIFaddend I propose in this work follows a generation from scratch of entire datasets suitable for the training of new algorithms, based on the 3D modelization of a region of human tissue at the cellular level. The entire traditional sectioning process, which is made on real histological samples, is recreated virtually on this virtual model. This yields pairs of synthetic images with their corresponding ground-truth. Using this technique one would be able to collect sufficient material for the training (the entire phase or the preliminary part) of a model, overcoming the shortage of hand-labeled data. \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend The 3D modeling of a region of particular human tissue is a very complex task, and it is almost impossible to capture all the physiological richness of a histological system. The models I implemented thus are inevitably less sophisticated respect the target biological structures. I'll show two models: one of pancreatic tissue and another of dermal tissue, besides all the tools I used and the choices I made during the designing phase.

In order to present organically all the steps of my work the thesis is organized in chapters as follows:

\begin{description}
    \item [1. Theoretical Background]. \hfill \\
    In this chapter, I will describe how real histological images are obtained and their digitalization process works. Afterward, I will introduce the reader to the Deep Learning framework, explaining the key elements of this discipline and how they work. Finally, I will dedicate a section to the image segmentation problem, and the state of the art of segmentation DL-based algorithms, with particular attention to the applications in the bio-medical field.

    \item [2. Technical Tools for Model Development]. \hfill \\
    I will dedicate this chapter to the thorough description of every technical tool I needed during the designing phase of this project. The development has required the harmonization of many different technologies and mathematical tools, some of which not so popular. In this chapter, I will also describe my working environment.

    \item [3. Tissue Models Development]. \hfill \\
    This third chapter is the heart of the project. I will describe in detail all the steps necessary to create the two models, one of pancreatic tissue and the other of dermal tissue, and how I am able to produce the synthetic images. The first section is devoted to the modeling of the histological structures, while the second in entirely dedicated to the sectioning process and the subsequent refinements to the images.
\end{description}

    \newpage \ \newpage

    \chapter{Theoretical Background}
        In this first chapter, I will depict the theoretical context of the work. Section \ref{ssec:hist_im} will be dedicated to histological images, and the different techniques used to prepare the samples to analyze. Histological images recover a fundamental role in medicine and are the pillar of many diagnosis techniques. This discipline borns traditionally from the optical inspection of the tissue slides using a microscope, and it is gradually developing and improving with the advent of computers and digital image processing. It is important tough to understand how the samples are physically prepared, the final target of this work is in fact the virtual reconstruction of this process. In section \ref{ssec:DL} I will introduce the Deep Learning framework and describe how a Neural Network works and actually learns. The most advanced techniques for the automatic image processing implement Deep Learning algorithm, and understanding the general rules behind this discipline is crucial for a good comprehension of this work. In section \ref{ssec:segmentation} I will discuss in particular the problem of image segmentation and how it is tackled with different Neural Network architectures, showing what it is the state of the art of this research field.

        \section{Histological Images} \label{ssec:hist_im}

Modern histopathology is essentially based on the careful interpretation of microscopic images, with the intention of correctly diagnose patients and to guide therapeutic decisions. In the last years, thanks to the quick development of scanning techniques and image processing, the discipline of histology have seen radical improvements: the main of which undoubtedly is the passage from the microscope's oculars to the computer's screen. This digitalization process has brought several advantages, that were previously impossible in classical histology, like telepathology and remote assistance in diagnosis processes, the integration with other digitalized clinical workflows, and patients' history, and most importantly the opening to applications of artificial intelligence.
The name Whole Slide Imaging (WSI) refers to the modern virtual microscopy discipline, which consists of scanning a complete microscope slide and creating a single high-resolution digital file. This is commonly achieved by capturing many small high-resolution image tiles or strips and then montaging them to create a full image of a histological section. The four key steps of this process are image acquisition (scansion), editing, and on-screen image visualization.

In the field of Digital Pathology (DP) an essential concept in image understanding is the magnification factor, which indicates the scale of representation of the image and allows dimension referencing. This factor is usually indicated as the magnification power of the microscope's lenses used during the analysis. After the digitalization process, this original magnification factor is prone to change, depending on the resolution of the visualization screen. Therefore, image resolution is measured in $\mu m$ per pixel, and it is set by the different composition of the acquisition chain, as the optical sensor and the lenses. Histological scanner are usually equipped with 20$\times$ or 40$\times$ objectives, which correspond to 0.5 and 0.25 $mm$/pixels resolution values. Lenses with 20$\times$ magnification factor are the most suitable for the great majority of histopathological evaluations, and it is the golden standard for scansions, for its good trade-off between image quality and time of acquisition. Scansions with 40$\times$ magnification could increase four-fold acquisition and processing time, final file's dimension, and storage cost. A single WSI image, acquired with 20$\times$ will occupy more than 600 MB alone.

\begin{figure}
    \centering
    \includegraphics[width = \textwidth]{images/patches_grid}
    \caption{An example of whole slide image, with its grid decomposition in patches. It is visible the correspondence between a region of intereste maually anotated and the patches that matches that region. From \cite{WSI_grid}}
    \label{fig:patches_grid}
\end{figure}

Despite the WSI is a relatively mature discipline, it still struggles to integrate itself in the standard primitive diagnosis phase in histopathological laboratories. This is primarily due to some disadvantages, like images'resolution, image compression's artifacts, and auto-focusing algorithms, which plays a key role in the specimen interpretation. Furthermore, the scansion of histological samples is an additional step in the analysis which takes time. Despite the technological improvements the average time for the acquisition of a sample is around 5/10 minutes, depending on the number of slices in the slide, for just a single level of magnification. While in traditional histology, the pathologist has access to all the magnification levels at the same time. The real advantage, in fact, is in the long term. Once the images have been acquired they can be archived and consulted remotely almost instantaneously, helping clinical analysis and allowing remote assistance (telemedicine). Furthermore, the images now can be processed by artificial intelligence algorithms, allowing the application of technologies like Deep Learning which could revolutionize the research field, as already has been on many different disciplines in the scientific world.

In order to allow to automatically process, such big images as the ones obtained through WSI, it is necessary to subdivide them in smaller patches. The dimension of which should be big enough to allow interpretation and to preserve a certain degree of representability of the original image. In Figure \ref{fig:patches_grid} is shown an example of whole slide image, with its grid decomposition in patches. If the patches are too small, it should be over-specified for a particular region of tissue, loosing its general features. This could lead the learning algorithm to misinterpretation. However, this is not an exclusive limit of digital pathology, for a human pathologist would be impossible too to make solid decisions on a too limited sample of tissue. After the subdivision in patches, a typical process for biomedical images is the so-called \textit{data augmentation} of images, that is the process of creating re-newed images from the starting material through simple geometrical transformations, like translation, rotation, reflection, zoom in/out.

The analysis of histological images usually consists in detecting the different components in the samples and to recognise their arrangment as an healthy or pathological pattern. It is necessary to recognize every sign of vitality of the cells, evaluating the state of the nucleus. There are many addittional indicators to consider like the presence of inflammatory cells or tumoral cells. Furthermore, samples taken from different part of the human body present completely different characteristics, and this increase greatly the complexity of the analysis.
A reliable esamination of a sample thus require a careful inspection made by an high qualified expert. The automatization of this procedure would be extremily helpful, giving an increadible boost both in timing and in accessibility. However, this is not a simple task and in section \ref{ssec:soa_seg} I will show some actual model for biomedical image processing in detail.

 \begin{figure}
     \centering
     \includegraphics[width = 0.3\textwidth]{images/h&e_retyna}
     \caption{A sample of tissue from a retina (a part of the eye) stained with hematoxylin and eosin, cell nuclei stained blue-purple and extracellular material stained pink.}
     \label{fig:he_retyna}
 \end{figure}

\subsection{Slides Preparation for Optic Microscopic Observation} \label{ssec:samp_prep}
In modern, as in traditional, histology regardless on the final support of the image the slide has to be physically prepared, starting from the sample of tissue. The sample and slide preparation is a crucial step for histological or cytological observation. It is essential to highlight what needs to be observed and to \textit{immobilize} the sample at a particular point in time and with characteristics close to those of its living state. There are five key steps for the preparation of samples \cite{Alturkistani2015}:
\begin{description}
    \item [1) Fixation] is carried out immediately after the removal of the sample to be observed. It is used to immobilize and preserve the sample permanently in as life-like state as possible. It can be performed immersing the biological material in a formalin solution or by freezing, so immersing the sample in a tissue freezing medium which is then cooled in liquid nitrogen.

    \item [2) Embedding] if the sample has been stabilazed in a fixative solution, this is the subsequent step. It consists in hardening the sample in a paraffin embedding medium, in order to be able to carry out the sectioning. It is necessary to dehydrate the sample beforehand, by replacing the water molecules in the sample with ethanol.

    \item [3) Sectioning] Sectioning is performed using microtomy or cryotomy. Sectioning is an important step for the preparation of slides as it ensures a proper observation of the sample by microscopy. Paraffin-embedded samples are cut by cross section, using a microtome, into thin slices of 5 $\mu m$. Frozen samples are cut using a cryostat. The frozen sections are then placed on a glass slide for storage at -80$\degree$C. The choice of these preparation conditions is crucial in order to minimize the artifacts. Paraffin embedding is favored for preserving tissues; freezing is more suitable for preserving DNA and RNA and for the labeling of water-soluble elements or of those sensitive to the fixation medium.

    \item [4) Staining] Staining increases contrasts in order to recognize and differentiate the different components of the biological material. The sample is first deparaffinized and rehydrated so that polar dyes can impregnate the tissues. The different dyes can thus interact with the components to be stained according to their affinities. Once staining is completed, the slide is rinsed and dehydrated for the mounting step.

\end{description}

Hematoxylin and eosin stain (H\&E) is one of the principal tissue stains used in histology \cite{he_stain}, and it is the most widely used stain in medical diagnosis and is often the gold standard \cite{Rosai2007}. H\&E is the combination of two histological stains: hematoxylin and eosin. The hematoxylin stains cell nuclei blue, and eosin stains the extracellular matrix and cytoplasm pink, with other structures taking on different shades, hues, and combinations of these colors. An example of H\&E stained is shown in Figure \ref{fig:he_retyna}, in which we can see the typical colour palette of an histological specimen.

        \clearpage
        \section{Introduction to Deep Learning} \label{ssec:DL}
Deep Learning is part of the broader framework of Machine Learning and Artificial Intelligence. Indeed all the problems typically faced using ML can also be addressed with DL techniques, for instance, regression, classification, clustering, and segmentation problems. We can think of DL as a universal methodology for iterative function approximation with a great level of complexity. In the last decades, this technology has seen a frenetic diffusion and an incredible development, thanks to the always increasing available computational power, and it has become a staple tool in all sorts of scientific applications.

\subsection{Perceptrons and Multilayer Feedforward Architecture}
Like other artificial learning techniques, DL models aim to \textit{learn} a relationship between some sort of input and a specific kind of output. In other words,  approximating numerically the function that processes the input data and produces the desired response. For example, one could be interested in clustering data in a multidimensional features space, or the detection of objects in a picture, or text manipulation/generation. The function is approximated employing a greatly complex network of simple linear and non-linear mathematical operations arranged in a so-called Neural Network (typically with millions of parameters). The seed idea behind this discipline is to recreate the functioning of actual neurons in the human brain: their entangled connection system and their ``ON/OFF" behavior \cite{10.5555/3275328}.

The fondamental unit of a neural network is called perceptron, and it acts as a digital counterpart of a human neuron. As shown in Figure \ref{fig:perceptron} a perceptron collects in input a series on $n$ numerical signals $\vec{x} = 1, x_1, ..., x_n $ and computes a linear wieghted combination with the weights vectors $\vec{w} = w_0, w_1, ..., w_n$, where $w_0$ is a bias factor:

\begin{equation}
    f(\vec{x},\vec{w}) = \chi(\vec{x} \cdot \vec{w}).
\end{equation}

The results of this linear combination are given as input to a non-linear function $\chi(x)$ called the activation function. Typical choices as activation function are any sigmoidal function like $sign(x)$ and $tanh(x)$, but in more advanced applications other functions like ReLU \cite{1803.08375} are used. The resulting function $f(\vec{x},\vec{w})$ has then a simple non linear behaviour. It produces a binary output: 1 if the weighted combination is high enough and 0 if it is low enough, with a smooth modulation in-between the two values.

\begin{figure}
    \centering
    \includegraphics[width = 0.7\textwidth]{images/perceptron}
    \caption{Schematic picture of a single layer perceptron. The input vector is linearly combined with the bias factor and sent to an activation function to produce the numerical "binary" output.}
    \label{fig:perceptron}
\end{figure}

The most common architecture for a NN is the so-called \textit{feed-forward} architecture, where many individual perceptrons are arranged in chained layers, which take as input the output of previous layers along with a straight information flux. More complex architectures could implements also recursive connection, linking a layer to itself, but it should be regarded as sophistication to the standard case. There are endless possibilities of combination and arrangement of neurons inside a NN's layer, but the most simple ones are known as \textit{fully-connected} layers, where every neuron is linked with each other neuron of the following layer, as shown in Figure \ref{fig:fully_connected}. Each connection has its weight, which contributes to modulate the overall combination of signals. The training of a NN consists then in the adjustment and fine-tuning of all the network's weights and parameters through iterative techniques until the desired precision in the output generation is reached.

\begin{figure}
    \centering
    \includegraphics[width = 0.4\textwidth]{images/fully_connected}
    \caption{Schematic representation of a fully connected (or dense) layer. Every neuron from the first layer is connected with every output neuron. The link thickness represent the absolute value of the combination weight for that particular value. }
    \label{fig:fully_connected}
\end{figure}

Although a fully connected network represents the simplest linking choice, the insertion of each weight increases the number of overall parameters, and so the complexity of the model. Thus we want to create links between neurons smartly, rejecting the less useful ones. Depending on the type of data under analysis there are many different established typologies of layers. For example, in the image processing field, the most common choice is the convolutional layer, which implements a sort of discrete convolution on the input data, as shown in Figure \ref{fig:convolutional}. While processing images, the convolution operation confers to the perception of correlation between adjacent pixels of an image and their color channels, allowing a sort of spatial awareness. Furthermore, the majority of traditional computer vision techniques are based on the discrete convolution of images, and on the features extracted from them.

As a matter of principle a NN with just two successive layers, which is called a \textit{shallow} network, and with an arbitrary number of neurons per layer, can approximate arbitrary well any kind of smooth enough function \cite{pinkus_1999}. However, direct experience suggests that networks with multiple layers, called \textit{deep} networks, can reach equivalent results exploiting a lower number of parameters overall. This is the reason why this discipline goes under the name of \textit{deep} learning: it focuses on deep networks with up to tents of hidden layers. Such deep structures allow the computation of what is called deep features, so features of the features of the input data, that allows the network to easily manage concepts that would be bearly understandable for humans.

\begin{figure}
    \centering
    \includegraphics[width = 0.7\textwidth]{images/CL}
    \caption{Schematical representation of a convolutional layer. The input data are processed by a window kernel that slides all over the image. This operation can recreate almost all the traditional computer vision techniques, and can overcome them, creating new operations, which would be unthinkable to hand-engineered.}
    \label{fig:convolutional}
\end{figure}

\subsection{Training of a NN -  Error Back-Propagation}
Depending on the task the NN is designed for, it will have a different architecture and number of parameters. Those parameters are initialized to completely random values, tough. The training process is exactly the process of seeking iteratively the right values to assign to each parameter in the network in order to accomplish the task. The best start to understanding the training procedure is to look at how a supervised problem is solved. In supervised problems, we start with a series of examples of true connections between inputs and correspondent outputs and we try to generalize the rule behind those examples. After the rule has been picked up the final aim is to exploit it and to apply it to unknown data, so the new problem could be solved. In opposition to the concept of supervised problems, there are the \textit{unsupervised} problems, where the algorithm does not try to learn a rule from a practical example but try to devise it from scratch. A task typically posed as unsupervised is clustering, when different data are separated in groups based on the values of their features in the feature space. Usually, only the number of groups is taken in input from the algorithm, and the subdivision is completely performed by the machine. In the real world, by the way, there are many different and creative shapes between pure supervised and pure unsupervised learning, based on the actual availability of data and specific limitations to the individual task.

\begin{figure}
    \centering
    \includegraphics[width = 0.6\textwidth]{images/cifar10}
    \caption{Sample grid of images from the CIFAR10 dataset. Each one of the 32 $\times$ 32 image is labeled with one of the ten classes of objects: \textit{plane, car, bird, cat, deer, dog, frog, horse, ship, truck}.}
    \label{fig:cf10}
\end{figure}

An interesting mention in this regards should be made about \textit{semi-supervised} learning, which is typically used in bio-medical applications. This learning technique combines a large quantity of unlabeled data during training with a limited number of pre-labeled example. The blend between data can produce considerable improvement in learning accuracy, and leading to better results respect to pure unsupervised techniques. The typical situation of usage of this technique is when the acquisition of labeled data requires a highly trained human agent (as an anatomo-pathologist) or a complex physical experiment. The actual cost of building entire and suitable fully labeled training sets in these situations would be unbearable, and semi-supervised learnign comes in great practical help.

A good example of supervised problems tough is the classification of images. Let's assume we have a whole dataset of pictures of different objects (as cats, dogs, cars, etc.) like the CIFAR10 \cite{cifar10} dataset. This famous dataset is made of over 60$K$ labeled colored images 32$\times$32 divided into 10 categories of objects as shown in Figure \ref{fig:cf10}. We could be interested in the creation of a NN able to assign at every image its belonging class. This NN could be arbitrarily complex but it certainly will take as input a 32$\times$32$\times$3 RGB image and the output will be the predicted class. A typical output for this problem would be a probability distribution over all the 10 classes like:

\begin{align}
    \vec p & = (p_1, p_2, \dots, p_{10}), \\
    &\sum_{i=1}^{10} p_i = 1,
\end{align}
and it should be compared with the true label, that is represented just as a binary sequence $\vec t$ with the bit correspondent to the belonging class set as 1, and all the others value set to 0:

\begin{equation}
    \vec t = (0,0,\dots, 1, \dots,0,0).
\end{equation}

Every time an image is given to the model an estimate of the output is produced. Thus, we need to measure the \textit{distance} between that prediction and the true value, to quantify the error made by the algorithm and try to improve the model's predictive power. The functions used for this purpose are called loss functions. The most common choice is the Mean Squared Error (MSE) function that is simply the averaged $L^2$ norm of the difference vector between $\vec p$ and $\vec t$:

\begin{equation}
    MSE = \frac{1}{n} \sum_{i=0}^{n} (t_i - p_i)^2.
    \label{eq:MSE}
\end{equation}

Let's say the NN under training has $L$ consecutive layers, each one with its activation function $f^k$ and its weights vector $\vec w^k$, hence the prediction vector $\vec p$ could be seen as the result of the concesutive, nested, application through all the layers:

\begin{equation}
    \vec p = f^L(\vec w^L \cdot (f^{L-1}(\vec w^{L-1} \cdot ... \cdot f^1(\vec w^1 \cdot \vec x)))).
    \label{eq:neste_layers}
\end{equation}

From both equations \ref{eq:MSE} and \ref{eq:neste_layers} it is clear that the loss function could be seen as a function of all the weights vectors of every layer of the network. So if we want to reduce the distance between the NN prediction and the true value we need to modify those weights to minimize the loss function. The most established algorithm to do so for a supervised task in a feed-forward network is the so-called \textit{error back-propagation}.

The back-propagation method is an iterative technique that works essentially computing the gradient of the loss function with respect to the weights using the derivative chain rule and updating by a small amount the value of each parameter to lower the overall loss function at each step. Each weight is  \textit{moved} counter-gradient, and summing all the contribution to every parameter the loss function approaches its minimum. In equation \ref{eq:weight_update} is represented the variation applied to the $j^{th}$ weight in the $i^{th}$ layer in a single step of the method:

\begin{equation}
    \Delta w_{ij} = - \eta \frac{\partial E}{\partial w_{ij}},
    \label{eq:weight_update}
\end{equation}
where $E$ is the error function, and $\eta$ is the \textit{learning coefficient}, that modulate the effect of learning through all the training process. This iterative procedure is applied completely to each image in the training set several times, each time the whole dataset is reprocessed is called an \textit{epoch}. The great majority of the dataset is exploited in the training phase to keep running this trial and error process and just a small portion is left out (typically 10\% of the data) for a final performance test.

The loss function shall inevitably be differentiable, and its behavior heavily influences the success of the training. If the loss function presents a gradient landscape rich of local minima the gradient descent process would probably get stuck in one of them. More sophisticated algorithms capable of avoiding this issue have been devised, with the insertion of some degree of randomness in them, as the Stochastic Gradient Descent algorithm, or the wide used \textit{Adam} optimizer \cite{1412.6980}.

While Error-Back Propagation is the most established standard in DL applications, it suffers from some problems. The most common one is the so-called vanishing or exploding gradient issue, which is due to the iterative chain derivation through all the nested level of composition of the function. Withouth a careful choice of the right activation function and the tuning of the learning hyper-parameters it is very easy to bump into this pitfall. Furthermore the heavy use of derivation rises the inability to handle non-differentiable components and hinders the possibility of parallel computation. However, there are many alternative approaches to network learning beside EBP. The Minimization with Auxiliary Variables (MAV) method builds upon previously proposed methods that break the nested objective into easier-to-solve local subproblems via inserting auxiliary variables corresponding to activations in each layer. Such method avoids gradient chain computation and the potential issues associated with it \cite{1806.09077}. A further alternative approach to train the network is the Local Error Signals (LES), which is based on layer-wise loss functions. In \cite{1901.06656}, is shown that layer-wise training can approach the state-of-the-art on a variety of image datasets. It is used a single-layer sub-networks and two different supervised loss functions to generate local error signals for the hidden layers, and it is shown that the combination of these losses helps with optimization in the context of local learning.

The training phase is the pulsing heart of a DL model development and it could take even weeks on top-level computers for the most complicated networks. In fact, one of the great limits to the complexity of a network during the designing phase is exactly the available computational power. There are many more further technical details necessary for proper training, the adjustment of which can heavily impact the quality of the algorithm. However, after the training phase, we need to test the performance of the NN. This is usually done running the trained algorithm on never seen before inputs (the test dataset) and comparing the prediction with the ground-truth value. A good way to evaluate the quality of the results is to use the same function used as the loss function during the training, but there is no technical restriction to the choice of this quality metric. The average score on the whole test set is then used as a numerical score for the network, and it allows straightforward comparison with other models' performances, trained for the same task. All this training procedure is coherently customized to every different application, depending on which the problem is posed as supervised or not and depending on the more or less complex network's architecture. The leitmotif is always finding a suitable loss function that quantifies how well the network does what it has been designed to do and trying to minimize it, operating on the parameters that define the network structure.

        \clearpage
        \section{Deep Learning-Based Segmentation Algorithms} \label{ssec:segmentation}
In digital image processing, image segmentation is the process of recognizing and subdividing an image into different regions of pixels that show similar features, like color, texture, or intensity. Typically, the task of segmentation is to recognize the edges and boundaries of the different objects in the image and assigning a different label to every detected region. The result of the segmentation process is an image with the same dimensions of the starting one made of solid color regions, representing the detected objects. This image is called \textit{segmentation mask}. In Figure \ref{fig:seg_example} is shown an example of segmentation of a picture of an urban landscape: different colors are linked to different classes of objects like persons in magenta and scooters in blue. This technology has a significant role in a wide variety of application fields such as scene understanding, medical image analysis, augmented reality, etc.

\begin{figure}
    \centering
    \includegraphics[width = \textwidth]{images/seg_example}
    \caption{Example of the resulting segmentation mask of an image of an urban landscape. Every interesting object of the image is detected and a solid color region replaces it in the segmentation mask. Every color corresponds to a different class of objects, for example, persons are highlighted in magenta and scooters in blue. The shape and the boundaries of every region should match as precisely as possible the edges of the objects.}
    \label{fig:seg_example}
\end{figure}

A relatively easy segmentation problem, and one of the first to be tackled, could be distinguishing an object from the background in a grey-scale image, like in Figure \ref{fig:fing_prints}. The easiest technique to perform segmentation in this kind of problem is based on thresholding. Thresholding is a binarization technique based on the image's grey-level histogram: to every pixel with luminosity above that threshold is assigned the color \textit{white}, and vice versa the color \textit{black}. However, this is a very primitive and fallacious, yet very fast method, and it manages poorly complex images or images with un-uniformity in the background.

A lot of other traditional techniques improve this first segmentation method \cite{Chouhan2018}. Some are based on the object's edges recognition, exploiting the sharp change in luminosity typically in correspondence of the boundary of a shape. Other techniques exploit instead a region-growing technology, according to which some \textit{seed} region markers are scattered on the image, and the regions corresponding to the objects in the image are grown to incorporate adjacent pixels with similar properties.

\begin{figure}
    \centering
    \includegraphics[width = 0.4\textwidth]{images/fingerprints}
    \caption{Example of the resulting segmentation mask of an image of a fingeprint obtained trhough a thresholding algorithm. The result is not extremely good, but this techinque is very easy to implement and runs very quickly.}
    \label{fig:fing_prints}
\end{figure}

Every development of traditional computer vision or of Machine Learning-based segmentation algorithm suffers from the same, inevitable limitation. For every one of those techniques is the designing phase in with the operator should decide precisely which features to extract from the image, like different directional derivatives in the image plane or image entropy, and how to process them for the rest of the analysis. There is thus an intrinsic limitation in the human comprehension of those quantities and in the possible way to combine them. The choice is made on the previous experimental results in other image processing works, and on their theoretical interpretation. Neural Networks instead are relieved from this limitation, allowing themselves to learn which features are best suited for the task and how they should be processed during the training phase. The complexity is then moved on to the design of the DL model and on its learning phase rather than on the hand-engineering design of the features to extract. In Figure \ref{fig:nuclei_feature} an example high-level feature extracted from a DL model trained for the segmentation of nuclei in a histological sample. The model learns the typical pattern of arrangement of nuclei, which would have been impossible to describe equally in advance.

\begin{figure}
    \centering
    \includegraphics[width = 0.5\textwidth]{images/nuclei_feature}
    \caption{(A) An extract from an histological samples, used as input image for the model. (B) The exact segmentation mask. (C)
    Example of accentuated features during the training: (1-4) for back-ground recognition, (5-8) for nuclei detection. From \cite{alheejawi2020deep}.}
    \label{fig:nuclei_feature}
\end{figure}

\subsection{State of the Art on Deep Learning Segmentation} \label{ssec:soa_seg}
Similarly to many other traditional tasks, also for segmentation, there has been a thriving development lead by the diffusion of deep learning, that boosted the performances resulting in what many regards as a paradigm shift in the field \cite{deep_seg_SOA}.

In further detail, image segmentation can be formulated as a classification problem of pixels with semantic labels (semantic segmentation) or partitioning of individual objects (instance segmentation). Semantic segmentation performs pixel-level labeling with a set of object categories (e.g. boat, car, person, tree) for all the pixels in the image, hence it is typically a harder task than image classification, which requires just a single label for the whole image. Instance segmentation extends semantic segmentation scope further by detecting and delineating each object of interest in the image (e.g. partitioning of individual nuclei in a histological image).

There are many prominent Neural Network architectures used in the computer vision community nowadays, based on very different ideas such as convolution, recursion, dimensionality reduction, and image generation. This section will provide an overview of the state of the art of this technology and will dwell briefly on the details behind some of those innovative architectures.

\begin{description}
    \item [Recurrent Neural Networks (RNNs) and the LSTM] \hfill \\
        The typical application for RNN is processing sequential data, as written text, speech or video clips, or any other kind of time-series signal. In this kind of data, there is a strong dependency between values at a given time/position and values previously processed. Those models try to implement the concept of \textit{memory} weaving connections, outside the main information flow of the network, with the previous NN's input. At each time-stamp, the model collects the input from the current time $X_i$ and the hidden state from the previous step $h_{i-1}$ and outputs a target value and a new hidden state (Figure \ref{fig:recNN}). Typically RNN cannot manage easily long-term dependencies in long sequences of signals. There is no theoretical limitation in this direction, but often it arises vanishing (or exploding) gradient problematics during the training phase. A specific type of RNN has been designed to avoid this situation, the so-called Long Short Term Memory (LSTM) \cite{LSTM}. The LSTM architecture includes three gates (input gate, output gate, forget gate), which regulate the flow of information into and out from a memory cell, which stores values over arbitrary time intervals.

        \begin{figure}
            \centering
            \includegraphics[width = 0.8\textwidth]{images/recNN}
            \caption{Example of the structure of a simple Recurrent Neural Network from  \cite{deep_seg_SOA}.}
            \label{fig:recNN}
        \end{figure}

    \item [Encoder-Decoder and Auto-Encoder Models] \hfill \\
        Encoder-Decoder models try to learn the relation between an input and the corresponding output with a two steps process. The first step is the so-called \textit{encoding} process, in which the input $x$ is compressed in what is called the \textit{latent-space} representation $z = f(x)$. The second step is the \textit{decoding} process, where the NN predicts the output starting from the latent-space representation $ y = g(z)$. The idea underneath this approach is to capture in the latent-space representation the underlying semantic information of the input that is useful for predicting the output. ED models are widely used in image-to-image problems (where both input and output are images) and for sequential-data processing (like Natural Language Processing, NLP). In Figure \ref{fig:EDNN} is shown a schematic representation of this architecture. Usually, these model follow a supervised training, trying to reduce the reconstruction loss between the predicted output and the ground-truth output provided while training. Typical applications for this technology are image-enhancing techniques like de-noising or super-resolution, where the output image is an improved version of the input image. Or image generation problems (e.g. plausible new human faces generation) in which all the properties which define the type of image under analysis should be learned in the representation latent space.

        \begin{figure}
            \centering
            \includegraphics[width = 0.8\textwidth]{images/EDnet}
            \caption{Example of the structure of a simple Encoder-Decoder Neural Network from  \cite{deep_seg_SOA}.}
            \label{fig:EDNN}
        \end{figure}

    \item [Generative Adversarial Networks (GANs)] \hfill \\
        The peculiarity of Generative Adversarial Network (GAN) lies in its structure. It is actually made of two distinct and independent modules: a generator and a discriminator, as shown in Figure \ref{fig:GAN}. The first module $G$, responsible for the generation, typically learns to map a prior random distribution of input $z$ to a target distribution $y$, as similar as possible to the target $G = z \rightarrow y$ (i.e. almost any kind of image-to-image problem could be addressed with GANs, as in \cite{1611.07004}). The second module, the discriminator $D$, instead is trained to distinguish between \textit{real} and \textit{fake} images of the target category. These two networks are trained alternately in the same training process. The generator tries to fool the discriminator and vice versa. The name adversarial is actually due to this \textit{competition} within different parts of the network. The formal manner to set up this adversarial training lies in the accurate choice of a suitable loss function, that will look like: $$L_{GAN} = \mathbb{E}_{x \sim p_{data}(x)}[logD(x)] + \mathbb{E}_{z \sim p_{z}(z)}[log(1-D(G(z)))]$$.
        The GAN is thus based on a min-max game between $G$ and $D$. $D$ aims to reduce the classification error in distinguishing fake samples from real ones, and as a consequence maximizing the $L_{GAN}$. On the other hand, $G$ wants to maximize the $D$'s error, hence minimizing $L_{GAN}$. The result of the training process is the trained generator $G^*$, capable of produce an arbitrary number of new data (images, text, or whatever else): $$ G^* = \operatorname*{arg\,min_Gmax_D} L_{GAN}$$.
        This peculiar architecture has yielded several interesting results and it has been developed in many different directions, with influences and contaminations with other architectures \cite{1611.07004}.

        \begin{figure}
            \centering
            \includegraphics[width = 0.5\textwidth]{images/GAN}
            \caption{Schematical representation of a Generative Adversarial Networks, form \cite{deep_seg_SOA}.}
            \label{fig:GAN}
        \end{figure}

    \item [Convolutional Neural Networks (CNNs)] \hfill \\
        As stated before CNNs are a staple choice in image processing DL applications. They mainly consist of three types of layers:

        \begin{enumerate}[i]
            \item convolutional layers, where a kernel window of parameters is convolved with the image pixels and produce numerical features maps.

            \item nonlinear layers, which apply an activation function on feature maps (usually element-wise). This step allows the network to introduce non-linear behavior and then increasing its modeling capabilities.

            \item pooling layers, which replace a small neighborhood of a feature map with some statistical information (mean, max, etc.) about the neighborhood and reduce the spatial resolution.
        \end{enumerate}

        Given the arrangement of successive layers, each unit receives weighted inputs from a small neighborhood, known as the receptive field, of units in the previous layer. The stack of layers allows the NN to perceive different resolutions: the higher-level layers learn features from increasingly wider receptive fields. The leading computational advantage given by CNN architecture lies in the sharing of kernels' weights within a convolutional layer. The result is a significantly smaller number of parameters than fully-connected neural networks. In section \ref{ssec:sttrNN} will be shown a particular application of this architecture, known as \textit{style-transfer} network, which is a particular algorithm capable of implanting the visual texture of a \textit{style} image onto the content of a different image, producing interesting hybrid images. Some of the most notorious CNN architectures include: AlexNet \cite{AlexNet}, VGGNet \cite{1409.1556}, and U-Net \cite{U-net}.
\end{description}

For this work, U-net architecture is particularly interesting. The U-net model was initially developed for biomedical image segmentation, and in its structure reflects characteristics of both CNN and Encoder-Decoder models. Ronneberger et al.\cite{U-net} proposed this model for segmenting biological microscopy images in 2015. The U-Net architecture is made of two branches, a contracting path to capture context, and a symmetric expanding path (see Figure \ref{fig:unet}). The down-sampling flow is made of a Fully Convolutional Network (FCN)-like architecture that computes features with 3 $\times$ 3 kernel convolutions. On the other hand, the up-sampling branch exploits up-convolution operations (or deconvolution), reducing the number of feature maps while increasing their dimensions. Another characteristic of this architecture is the presence of direct connections between layers of a similar level of compression in compressing and decompressing branches. Those links allow the NN to preserve spatial and pattern information. The Network flow eventually ends with a 1 $\times$ 1 convolution layer responsible for the generation of the segmentation mask of the input image.

    \begin{figure}
        \centering
        \includegraphics[width = 0.8\textwidth]{images/unet}
        \caption{Scheme of the typical architecture of a U-net NN. This particular model was firstly proposed by Ronneberger \textit{et al.} in \cite{U-net}.}
        \label{fig:unet}
    \end{figure}

A recent example of a practical application of a CNN to histological images could be found in \cite{Ki67}. In this work the \texttt{Inception v3} is trained on hand-labeled samples of pancreatic tissue (like in Figure \ref{fig:man_seg}) to recognise tumoral regions from helthy ones in a pancreatic tissue specimen treated with Ki67 staining. The Inception v3 \cite{ravindran2018classification} network is a deep convolutional network developed by Google, trained for object detection and image calssification on the ImageNet dataset \cite{5206848}. Recognition of tumoral region of Ki67 stained pancreatic tissue samples is based on the detection and counting of some specific marker cells. In Figure \ref{fig:autom_seg} is shown a pair of original image and the computed segmentation mask, which label in red tumoral regions and in green the healthy ones. This work is based on a technique called \texttt{transfer learning}, which consists in the customization and specialization of a pre-trained NN previously trained for similar, but essentially different, tasks. The final part of the training of this version of Inception v3 has been performed of a dataset of 33 whole slide images of Ki67 stained neuroendocrine tumor biopsies acquired from 33 different patients, digitized with a 20$\times$ magnification factor and successively divided in 64$\times$64 patches.

    \begin{figure}
        \centering
        \includegraphics[width = 0.4\textwidth]{images/PancTissue}
        \includegraphics[width = 0.4\textwidth]{images/PancTissueSeg}
        \caption{(left) Original image of a Ki67 stained pancreatic tissue sample, (right) the corresponding segmentation mask, which label in red tumoral regions and in green the healthy ones. From \cite{Ki67}.}
        \label{fig:autom_seg}
    \end{figure}

\subsection{Image Segmentation Datasets}
Besides the choice of suitable architecture the most important aspect while developing a NN is the dataset on which perform the training process. Let's confine the discussion only to image-to-image problems, like segmentation problems. There are a lot of widely used datasets, but I want to mention just a few of them to give the idea of their typical characteristics.

A good example of segmentation is the Cityscapes dataset \cite{Cityscapes}, which is a large-scale database with a focus on semantic understanding of urban street scenes. The dataset is made of video sequences from the point of view of a car in the road traffic, from 50 different cities in the world. The clips are made of 5K frames, labeled with extremely high quality at pixel-level and an additional set of 20K weakly-annotated frames. Each pixel in the segmentation mask contains the semantic classification, among over 30 classes of objects. An example of an image from this dataset is shown in Figure \ref{fig:seg_example}.

The PASCAL Visual Object Classes (VOC) \cite{PASCAL} is another of the most popular datasets in computer vision. This dataset is designed to support the training of algorithms for 5 different tasks: segmentation, classification, detection, person layout, and action recognition. In particular, for segmentation, there are over 20 classes of labeled objects (e.g. planes, bus, car, sofa, TV, dogs, person, etc.). The dataset comes divided into two portions: training and validation, with 1,464 and 1,449 images, respectively. In Figure \ref{fig:PASCAL} is shown an example of an image and its corresponding segmentation mask.

    \begin{figure}
        \centering
        \includegraphics[width = 0.5\textwidth]{images/PASCAL}
        \caption{An example image from the PASCAL dataset and its corresponding segmentation mask \cite{PASCAL}.}
        \label{fig:PASCAL}
    \end{figure}

As last mention I would report t he ImageNet project \cite{5206848}, which is a large visual database designed for use in visual object recognition software research. It consists of more than 14 million images have been hand-annotated by the project to indicate what objects are pictured and in at least one million of the images, bounding boxes are also provided. ImageNet contains more than 20,000 categories of objects. Since 2010, the ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes. This kind of competitions is very important for the research field, as it inspire and encourage the development of new models and architectures.

It is worth mentioning that in the medical image processing domain typically the available dataset is definitely not that rich and vast (that is actually the seed of this work) and thus many techniques of data augmentation have been devised, to get the best out of the restricted amount of material. Generally, data augmentation manipulates the starting material applying a set of transformation to create new material, like rotation, reflection, scaling, cropping and shifting, etc. Data augmentation has been proven to improve the efficacy of the training, making the model less prone to over-fitting, increasing the generalization power of the model, and helping the convergence to a stable solution during the training process.

        \clearpage

    \chapter{Technical Tools for Model Development} \label{sec:tech_tool}
        As mentioned in the introduction, this project wants to produce synthetic histological images paired with their corresponding segmentation mask, to train Neural Networks for the automatization of real histological images analysis. The production of artificial images passes through the processing of a three dimensional, virtual model of a histological structure, which is the heart of this thesis work. The detailed description of the development of the two proposed histological models will follow the present chapter and will occupy all the chapter \ref{sec:models}. Here I will dwell, instead, on every less common tool employed during the models' designing phase. From the practical point of view, this project is quite articulated and the development has required the harmonization of many different technologies, tools, and code libraries. The current chapter should be seen as a theoretical complement for chapter \ref{sec:models}, and its reading is suggested to the reader for any theoretical gap or for any further technical deepening. The reader already familiar with those technical tools should freely jump to the models' description.

All the code necessary for the work has been written in a pure \texttt{Python} environment, using several already established libraries and writing by my self the missing code for some specific applications. I decided to code in \texttt{Python} given the thriving variety of available libraries geared toward scientific computation, image processing, data analysis, and last but not least for its ease of use (compared to other programming languages). In each one of the following subsections I will mention the specific code libraries which have been employed in this project for every technical necessity.

        \section{Quaternions} \label{ssec:quat}
    Quaternions are, in mathematics, a number system that expands to four dimensions the complex numbers. They have been described for the first time by the famous mathematician William Rowan Hamilton in 1843. This number system define three independent \textit{imaginary} units $\bm{i}$, $\bm{j}$, $\bm{k}$ as in (\ref{eq:quat_rules}), which allows the general representation of a quaternion $\bm{q}$ is (\ref{eq:quat}) and its inverse $\bm{q}^{-1}$ (\ref{eq:inv_quat}) where $a,b,c,d$ are real numbers:

    \begin{align}
        \bm{i}^2 & = \bm{j}^2 = \bm{k}^2 = \bm{i}\bm{j}\bm{k} = -1, \label{eq:quat_rules}\\
        \bm{q} & = a + b\bm{i} + c\bm{j} + d\bm{k}, \label{eq:quat}\\
        \bm{q}^{-1 } = (a + b\bm{i} + c\bm{j} + d\bm{k})^{-1} & = \frac{1}{a^2 + b^2 + c^2 +d^2}\ (a - b\bm{i} - c\bm{j} - d\bm{k}). \label{eq:inv_quat}
    \end{align}

    Furthermore, the multiplication operation between quaternionn does not benefit from commutativity, hence the product between basis elements will behave as follows:

    \begin{align}
        \bm{i} \cdot 1 = 1 \cdot \bm{i} = \bm{i}, & \qquad  \bm{j} \cdot 1 = 1 \cdot \bm{j} = \bm{j}, \qquad \bm{k} \cdot 1 = 1 \cdot \bm{k} = \bm{k} \label{eq:Ham_prod}\\
        & \bm{i} \cdot \bm{j}= \bm{k}, \qquad \bm{j} \cdot \bm{i}= -\bm{k} \nonumber \\
        & \bm{k} \cdot \bm{i}= \bm{j}, \qquad \bm{i} \cdot \bm{k}= -\bm{j} \nonumber \\
        & \bm{j} \cdot \bm{k}= \bm{i}, \qquad \bm{k} \cdot \bm{j}= -\bm{i}. \nonumber
    \end{align}

    This number system has plenty of peculiar properties and applications, but for this project, quaternions are important for their ability to represent, in a very convenient way, rotations in three dimensions. The particular subset of quaternions with vanishing real part ($a=0$) has a useful, yet redundant, correspondence with the group of rotations in tridimensional space SO(3). Every 3D rotation of an object can be represented by a 3D vector $\vec u$: the vector's direction indicates the axis of rotation and the vector magnitude $|\vec u|$ express the angular extent of rotation. However, the matrix operation which expresses the rotation around an arbitrary vector $\vec u$ it is quite complex and does not scale easily for multiple rotations \cite{10.1007/BFb0031048}, which brings to very heavy and entangled computations.

    Using quaternions for expressing rotations in space, instead, it is very convinient. Given the unit rotation vector $\vec u$ and the rotation angle $\theta$, the corresponding rotation quaternion $\bm{q}$ becomes (\ref{eq:rot_quat}):

    \begin{align}
        \vec u & = (u_x, u_y, u_z) = u_x\bm{i} + u_y\bm{j} + u_z\bm{k}, \\
        \bm{q} & = e^{\frac{\theta}{2}(u_x\bm{i} + u_y\bm{j} + u_z\bm{k})} = \cos{\frac{\theta}{2}} + (u_x\bm{i} + u_y\bm{j} + u_z\bm{k})\sin{\frac{\theta}{2}}, \label{eq:rot_quat}\\
        \bm{q}^{-1} & = \cos{\frac{\theta}{2}} - (u_x\bm{i} + u_y\bm{j} + u_z\bm{k})\sin{\frac{\theta}{2}},
    \end{align}

    where in (\ref{eq:rot_quat}) we can clearly see a generalization of the Euler's formula for the exponential notation of complex numbers, which hold for quaternions. It can be shown that the application of the rotation represented by $\bm{q}$ on an arbitrary 3D vector $\vec v$ should be easily expressed as:

    \begin{equation}
        \vec v\,' = \bm{q} \vec v \bm{q}^{-1},
    \end{equation}
    using the Hamilton product defined on quaternions (\ref{eq:Ham_prod}). This rule raises a very convinient and an extremily scalable way to compute consecutive rotations in space. Given two independent and consecutive rotations represented by the two quaternions $\bm{q}$ and $\bm{p}$ applyed on the vector $\vec v$ the resulting rotated vector $\vec v\,'$ is simply yielded as:

    \begin{equation}
        \vec v\,' = \bm{p} ( \bm{q} \vec v \bm{q}^{-1} ) \bm{p}^{-1} = (\bm{p}\bm{q}) \vec v (\bm{q}\bm{p})^{-1},
    \end{equation}
    which essentially is the application of the rotation $\bm{r} = \bm{q}\bm{p}$ on the vector $\vec v$. This representation is completely coherent with the algebra of 3D rotations, which does not benefit from commutativity in turn.

    Given this property, quaternions are indeed widely used in all sorts of applications of digital 3D space design, as for simulations and videogame. The position of an object in the space in simulations is generally given by the application of several independent rotations, typically in the order of a tenth of rotations, which with quaternions is given easily by the product of simple objects. Every other alternative method would imply the use of matrix representation of rotations or other rotation systems as Euler's angles and would eventually make the computation prohibitive.

    The use of quaternions in this work will be justified in section \ref{ssec:panc_tis_mod}, while speaking of parametric L-systems in 3D space, used to build the backbone of the ramified structure of blood vessels in the reconstruction of a sample of pancreatic tissue.

    I was able to find many \texttt{Python} libraries for computation with quaternions, but the one I appreciated the most for its interface and ease of use was the \texttt{pyquaternion}. With this library, it's immediate the definition of a quaternion by its correspondent rotation vector, and the multiplication between quaternions is straightforward.

\section{Parametric L-Systems} \label{ssec:Lsys}
    Lindenmayer systems, or simply L-systems,  were conceived as a mathematical theory of plant development \cite{lindenmayer1968mathematical} in 1968 by Aristid Lindenmayer. Successively, a lot of geometrical interpretations of L-systems were proposed to make them a versatile instrument for modeling the morphology typical of plants and other organic structures. As a biologist, Lindenmayer studied different species of yeast and fungi and worked the growth patterns of various types of bacteria (e.g. as the \textit{cyanobacteria Anabaena catenula}). The main purpose for which L-systems were devised was to allow a formal description of the development of simple multicellular living organisms. Subsequently, the potentiality of these systems was expanded to describe higher-order plants and complex branching structures.

    An L-system is in general defined by an \textit{axiom} sequence and some development \textit{rules}, which are recursively applied to the sequence and lead its development. The original proposed L-system was fairly simple and shows really well the idea underneath:

    \begin{align}
        & axiom\ :\ A \nonumber \\
        & rules\ :\ (A \rightarrow AB),\quad(B \rightarrow A) \nonumber
    \end{align}

    where $A$ and $B$ could be any two different patterns in the morphology of an algae, or could be different bifurcations in a ramificated structure. The iterative application of the rules to the axiom sequence, let's say for 7 times, will produce the following sequence:

    \begin{align}
        n = 0 &\ : A \nonumber \\
        n = 1 &\ : AB \nonumber \\
        n = 2 &\ : ABA \nonumber \\
        n = 3 &\ : ABAAB \nonumber \\
        n = 4 &\ : ABAABABA \nonumber \\
        n = 5 &\ : ABAABABAABAAB \nonumber \\
        n = 6 &\ : ABAABABAABAABABAABABA \nonumber \\
        n = 7 &\ : ABAABABAABAABABAABABAABAABABAABAAB\,. \nonumber
    \end{align}

    This kind of tool, as will be shown also in \ref{ssec:panc_tis_mod}, is particularly suited for the creation of structures with fractal behavior, and it has been used in this work to create the backbone of the entangled bifurcation in blood vessels in the modelization of pancreatic tissue. In particular, there was the need for a fractal-like space-filling ramification as the one shown in Figures \ref{fig:bf_ls}.
%DIF < and \ref{fig:full_ram}.

    \begin{figure}
        \centering
        \includegraphics[width = 0.7\textwidth]{images/bf_ls}
        \caption{Growth pattern for the space-filling fract-like system, used to mimic the blood vassel bifurcations in sec \ref{ssec:panc_tis_mod}.}
        \label{fig:bf_ls}
    \end{figure}

    %DIF < MAYBE THIS PICTURE IS REDUNDANT
    %DIF <  \begin{figure}
    %DIF <      \centering
    %DIF <      \includegraphics[width = 0.7\textwidth]{images/full_ram}
    %DIF <      \caption{oijijo}
    %DIF <      \label{fig:full_ram}
    %DIF <  \end{figure}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD <     %%%
\DIFdelend The system in Figure \ref{fig:bf_ls} represent the successive ramification of a structure which grows adding segments gradually shorter, by a lenght ratio parameter $R$ and inclined of $\delta = \pm 85 \degree$ respect the previous branch. The axiom and the rules that produce this structure are the following:

    \begin{align}
        & axiom\ :\ A \label{eq:bif_rules} \\
        & rule_1\ :\ A \rightarrow â F(1)[+A][-A] \nonumber \\
        & rule_2\ :\ F(s) \rightarrow F(s\cdot R) \nonumber
    \end{align}
    where $A$ represent the start of a new branch  and $F(s)$ represent a branch of lenght $s$. The presence of a rule which acts dfferently depending on the target object, is an further sophistication respect to the standard L-system. For this reason these systems are called \textit{parametric} L-systems.

    The use of standard L-systems turned out to be widespread, and there were a lot of different \texttt{Python} libraries at my disposal for coding. By the way, parametric L-systems were not just as popular, and I was not able to find a reliable library on which to build my work. I decided then to code a parametric branching system able to recreate the structure with rules (\ref{eq:bif_rules}) at any desired level of iteration. Having created the tool I needed on my own I was able to add all the optional features I would have needed during the development, like an adjustable degree of angular noise in the branch generation.

\section{Voronoi Tassellation} \label{ssec:vor_tass}
    Voronoi diagrams, or Voronoi decompositions, are space-partitioning systems, which divides an $n$-dimensional Euclidian space into sub-regions depending on the proximity to a given set of objects. More precisely, given an $n$-dimensional space and $m$ starting point $p_1,\dots, p_m$ inside it, the whole space will be subdivided in $m$ adjacent regions. Every point of the space is assigned to the region correspondent to the nearest starting point. In Figure \ref{fig:vor_20} is shown a practical example of a Voronoi decomposition of a plane into 20 regions corresponding to the 20 starting points. Informal use of Voronoi diagrams can be traced back to Descartes in 1644, and many other mathematicians after him. But, Voronoi diagrams are named after Georgy Feodosievych Voronoy who defined and studied the general n-dimensional case in 1908 \cite{VoronoiNouvellesAD}.

    \begin{figure}
        \centering
        \includegraphics[width = 0.4\textwidth]{images/vor_20}
        \caption{Example of a Voronoi decomposition of a plane into 20 regions corresponding to 20 starting points.}
        \label{fig:vor_20}
    \end{figure}

    More precisely, let $X$ be a metric space and $d$ the distance defined on it. Let $K$ be the set of indices and let $(P_k)_{k\in K}$ be the tuple of sites in the space $X$. The $k^{th}$ Voronoi cell $R_k$, associated with the site $P_k$ is the set of all the points in $X$ whose distance to $P_k$ is smaller than the distance to any other site $P_j$, with $j\neq k$, or in other words:

    \begin{equation}
        R_k = \{x \in X\;|\;d(x,P_k) \le d(x,P_j)\; \forall j \in K, \;j\neq k \}, \label{eq:formal_Vor_def}
    \end{equation}
    depending on the notion of distance defined on the space $X$ the final redistribution in subregions will look very differently.

    In addition to the choice of the distance function, another fundamental factor is the distribution of sites in the space to be divided. If the points are chosen equally and homogenously distributed the final distribution will appear as a simple regular lattice, while a completely random distribution of points in the space will provide a decomposition in cells with very different shapes and volumes, as shown in Figure \ref{fig:diff_pt}. Interesting results concerning points from a semi-random distribution will be shown in section \ref{ssec:panc_tis_mod}, which leads to decomposition with a good richness in shapes but with the desired homogeneity in volumes.

    \begin{figure}
        \centering
        \includegraphics[width = 0.3\textwidth]{images/reg_pt}
        \includegraphics[width = 0.3\textwidth]{images/ran_pt}
        \caption{On the right an example of 2D Voronoi decomposition resulting from homogeneously distributed points in the plane. On the left the resulting decomposition obtained from randomly distributed points in the plane, from \cite{ALSAYEDNOOR201644}.}
        \label{fig:diff_pt}
    \end{figure}

    The Voronoi decomposition has been of great interest in this project for the division of a 3D space in subregions, to recreate the spatial distribution of cells in a sample of human tissue, as will be shown in section \ref{ssec:panc_tis_mod}. The formal definition of Voronoi regions (\ref{eq:formal_Vor_def}) ensures the convexity of each decomposition's tassel, which in three-dimensional space would be adjacent convex polyhedrons. Every tassel of the decomposition will be represented by a bounded 3-dimensional convex hull \footnote{See section \ref{ssec:pol_sec} for further details.}, with except for those most external cells which are unbounded and requires special attention while using.

    The most widespread tool for the computation of Voronoi decompositions in \texttt{Python} is contained in the \texttt{spatial} submodule of the famous library \texttt{SciPy} \cite{2020SciPy-NMeth}, which is a staple tool for an incredible variety of scientific algorithms. The \texttt{Voronoi} object from \texttt{Scipy} library offers a very efficient algorithm for space-partitioning, and it has been one of the pillars for the modelization of tissues. Unluckily this module does not easily allow to perform Voronoi decomposition with different definitions of distance functions $d$ other than the Euclidian distance, which would have allowed interesting studies.

\section{Saltelli Algorithm - Randon Number Generation} \label{ssec:saltelli}
    As mentioned in section \ref{ssec:vor_tass}, in this project there was the need for quasi-random number generation for the production of Voronoi tessellations. Quasi-random sequences (or low-discrepancy sequences) are patterns of numbers that emulate the behavior of uniform random distributions but have a more homogeneous and quick coverage of the sampling domain, which provides an important advantage in applications as in quasi-Monte Carlo integration techniques, as shown in Figure \ref{fig:Subrandom_2D}. In computer science there is not any possibility of recreating \textit{true} random sequences, hence any stochasticity is completely deterministic in its essence even if produced by very chaotic processes\footnote{A chaotic process is a deterministic process which has an extremely sensible dependence on its starting conditions. This property mimics very effectively the behavior of true random processes, which are intrinsically forbidden in computer science.}. Indeed, every algorithm for random number generation is completely repeatable given its starting status. Quasi-random sequences are completely deterministic too, but implements more \textit{regular}, well-behaved algorithms.

    \begin{figure}
        \centering
        \includegraphics[width = 0.7\textwidth]{images/Subrandom_2D}
        \caption{Coverage of the unit square with an additive quasirandom numbers sequence as in \ref{eq:ad_rec} (up) and for uniformly sampled random numbers (bottom). From left to right: 10, 100, 1000, 10000 points.}
        \label{fig:Subrandom_2D}
    \end{figure}

    A first good example to understand the concept of quasi-random generation could be an additive reccurrence, as the following:
    \begin{equation}
        s_{n+1} = ( s_n + \alpha ) \bmod 1,
        \label{eq:ad_rec}
    \end{equation}
    which for every seed element $s_0$ and real parameter $\alpha$ produced completely different sequences.

    In the bottom line of Figure \ref{fig:Subrandom_2D} is clearly visible the good and homogeneous coverage of the sampling domain, although it is strongly visible a regular pattern between points, which does not convey an \textit{organic} sensation at all. However, increasing the complexity of our very simple starting model \ref{eq:ad_rec} it is possible to overcome this \textit{artificial} appearance of sampled points and to produce very good samples.

    A notorious algorithm for quasi-random number generation is the Sobol sequence, introduced by the russian mathematician Ilya M. Sobol in 1967 \cite{SOBOL2001271}. In its work, Sobol wanted to construct a sequence $x_n$ of points in the $s$-dimensional unitary hybercube $I^s = [0,1]^s$ such as for any integrable function $f$:
    \begin{equation}
        \lim_{n\to\infty} \frac{1}{n} \sum_{i=1}^{n} f(x_i) = \int_{I_s} f.
    \end{equation}

    Sobol wanted to minimize the \textit{holes} in the sampled domain (which it could be shown to be a property that helps the convergence of the sequence) and minimize as well the \textit{holes} in every lower-dimension projection of the sampled points. The particularly good distributions that fulfill those requirements are known as $(t,m,s)$-nets and $(t,s)$-sequences in base $b$.

    To better understand them we need first to define the concept of $s$-interval in base $b$, which is a subset of $I_s$ such as:
    \begin{equation}
        E_s^b = \prod_{j=1}^{s} \Bigg[ \frac{a_j}{b^{\,d_j}}, \frac{a_j + 1}{b^{\,d_j}}\Bigg),
    \end{equation}
    where $a_j$ and $d_j$ are non-negative integers, and $a_j < b^{d_j}$ for all j in \{1, ...,s\}.

    Let be $t$ and $m$ two integers such as $0 \leq t \leq m$. A $(t,m,s)$-net in base $b$ is defined as a sequence $x_n$ of $b^m$ points of $I_s$ such that:
    \begin{equation}
        \mathbf{Card} \ \mathbf{P} \cap \{x_1, \dots, x_n \} = b^t
    \end{equation}
    for all the elementary  interval $\mathbf{P}$ in base $b$ of hypervolume $\lambda(\mathbf{P}) =  b^{t-m}$.

    Given a non-negative integer $t$, a $(t,s)$-sequence in base $b$ is an infinite sequence of points $x_n$ such that for all integers $k \geq 0$, $m \geq t$ the sequence $ \{ x_{kb^m}, \dots, x_{(k+1)b^m-1} \}$ is a $(t,m,s)$-net in base $b$.

    Sobol in his article described in particular $(t,m,s)$-net and $(t,s)$-sequence in base 2. A more thorough description of all the formal properties of those particular sequences could be found in \cite{SOBOL1976236}.

    In order to perform the actual sampling during the modelization, it has been used the \texttt{saltelli} module from the \texttt{SALib} library, which performs sampling in an $s$-dimensional space following the Saltelli algorithm, which is a specific improved version of the Sobol algorithms oriented toward the parameter sensitivity analysis \cite{SALTELLI2002280}, \cite{SALTELLI2010259}.

%DIF <  In section \ref{ssec:app_mkov}, in Figure \ref{fig:sampling_comparison} is shown a comparison between corresponding section obtained following different sampling methods for the
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \section{Planar Section of a Polyhedron} \label{ssec:pol_sec}
    As will be shown in section \ref{ssec:panc_tis_mod} a fundamental step for the functioning of the modelization is the planar section of a three-dimensional polyhedron. It turned out that there is no general rule to perform a planar section of a convex polyhedron with an arbitrary number of faces, respect to an arbitrary sectioning plane. Hence, I devised an algorithm to handle this task. In the case of a full intersection, the result of the sectioning process of a polyhedron is a polygonal surface, otherwise, it could be an empty set of points or a segment in case of particular tangency, but those two cases are not of interest to the model. \DIFaddbegin \DIFadd{This tool has been created from scratch by me in the preliminar design phase and also the implemented algorithm has been devised by me and chosen as the best option among other possibilities. It is not fruit of an extended and thorough formal analysis, hence it has not the prententious to be an extremely optimized algortihm. Nevertheless, this tool passed all the test I required and yielded the expected results.
}\DIFaddend 

    Given a convex polyhedron with $n$ vertices and a sectioning plane $p$, let $V$ be the set of all the vertices and $f_p(\vec x)$ the equation defining the plane. The algorithm is defined by the following steps:

    \begin{figure}
        \centering
        \includegraphics[width = 0.6\textwidth]{images/sec_pol}
        \caption{In this picture is shown an example of the application of the algorithm for the planar section on a polyhedron. The vertices are divided into two groups, with different colors yellow and green. All the possible lines between any couple of vertices picked from the two classes are drawn in dark gray. In Turquoise the resulting planar section, obtained as the convex hull containing all the intersections between the lines and the plane.}
        \label{fig:sec_pol}
    \end{figure}

    \begin{enumerate}
        \item Divide $V$ in two subsets: $A$ made of those vertices which lie above and $B$, made of those which lie below the sectioning plane. Like in \ref{eq:ab_bel}:
        \begin{align}
            A = \{ \vec v \in V \ |\ f_p(\vec v)\geq 0\} \label{eq:ab_bel} \\
            B = \{ \vec v \in V \ |\ f_p(\vec v)\leq 0\} \nonumber
        \end{align}
        If any of the two subsets tourns out to be empty the plane $p$ does not intersect the polyhedron, and the section is empty. $A$ and $B$ are represented in different colors in Figure \ref{fig:sec_pol}.

        \item Detect, and \textit{draw}, any possible line that crosses two points respectively from $A$ and $B$. If $n_A$ and $n_B$ are the numbers of points above and below the plane then there will be $n_A \times n_B$ possible lines. In Figure \ref{fig:sec_pol} all the lines between the two classes of points are drawn in dark gray.

        \item Detect $P$, the set of all the points from the intersection between the $n_A \times n_B$ lines from the previous step and the sectioning plane $p$. All these points will lie on the same plane, within the boundaries of the polygonal section.

        \item The final polygon is then yielded by computing the convex hull of the points in $P$. The convexity of the starting polyhedron in fact ensures the convexity of any section of the solid.
    \end{enumerate}

    The result of the algorithm is, as just stated, a convex hull, which in geometry is defined as the smallest convex envelope or convex closure of a set of points. In 2 dimensions is the smallest convex polygon containing a certain set of points in a plane (In Figure \ref{fig:conv_hull} is shown the so-called ``rubber band effect"), and in 3 dimensions it is the smallest convex polyhedron containing a set of points in the space.

    \begin{figure}
        \centering
        \includegraphics[width = 0.3\textwidth]{images/conv_hull}
        \caption{Representation of the convex hull of a bounded planar set of points. This particular enclosure goes under the name of "rubber band effect".}
        \label{fig:conv_hull}
    \end{figure}

    In \texttt{Python}, the most convenient way to work with convex hulls was to use the submodule \texttt{spatial.ConvexHull} from the \texttt{SciPy} library \cite{2020SciPy-NMeth}. This module allows also a convenient way for plotting images with \texttt{MatPlotLib}, which is the point of reference for plotting and image formation in \texttt{Python}.

\section{Perlin Noise} \label{ssec:perlin}
    Perlin noise is a widely used form of noise in computer graphics, which mimics very well natural and smooth fluctuations around a constant value. It has been developed by Ken Perlin in 1983, and it is now the staple tool for giving texture to object in virtual modelization, often considered the \textit{salt} of computer graphics texturization.
    The Perlin noise is a gradient-based algorithm defined on grid discretization of a $n$-dimensional space. The algorithm involves three subsequent steps:

    \begin{figure}
        \centering
        \begin{subfigure}[b]{0.3\textwidth}
             \centering
             \includegraphics[width = \textwidth]{images/grid_grad}
             \caption{}
             \label{fig:grid_grad}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.3\textwidth}
             \centering
             \includegraphics[width = \textwidth]{images/PerlinNoiseDotProducts}
             \caption{}
             \label{fig:dot_prod}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.3\textwidth}
             \centering
             \includegraphics[width = \textwidth]{images/PerlinNoiseInterpolated}
             \caption{}
             \label{fig:interp}
        \end{subfigure}
        \caption{The three main steps of the algorithm to produce Perlin noise. In \ref{fig:grid_grad} the plane discretization and the assignment of a gradient vector to every node of the grid. In \ref{fig:dot_prod} the computation of the dot product with all the points inside the discretization and in \ref{fig:interp} the interpolation of the values to create the final function.}
        \label{fig:perlin_alg}
    \end{figure}

    \begin{enumerate}
        \item The first step is to discretize the $n$-dimensional space in a regular lattice: the dimension of the grid will impact heavily on the scale of the noise. As in Figure \ref{fig:grid_grad} at every node of the grid is assigned a randomly oriented $n$-dimensional unitary gradient vector. This is the preliminary setup which will allow the computation of the actual noise function in every point of the space.

        \item Given the candidate point $\vec x$ between the grid nodes onto which evaluate the noise there are $2^n$ nearest grid nodes. For each one of these $2^n$ nodes, it is evaluated the distance vector from $\vec x$ as the offset between the two points. Then it is computed the dot product between every pair made of nearby gradient vectors and the offset vector. This operation should be thought of as made on every point in the lattice, as in Figure \ref{fig:dot_prod}, where at every point of the grid is represented just one of the $2^2 = 4$ series of dot products.

        \item The final step is the interpolation between the $2^n$ series of dot products. To perform the interpolation usually is used a function with vanishing first degree (and preferably also second degree) derivative in correspondence of the $2^n$ grid nodes \footnote{Usually are used functions with a sigmoidal behavior, like any smoothstep function, which is a family of very common items in computer graphics.}. This means that the noise function will pass through zero at every node and have a gradient equal to the pre-computed grid node gradient. These properties give Perlin noise its characteristic spatial scale and smoothness.
    \end{enumerate}

    In general, the final result of the algorithm is a smooth function with a random-like behavior that mimics really well an organic appearance, like in Figure \ref{fig:my_perlin}, with fluctuation around the value 0, with amplitude $ \in [ 0;1] $. The surface in Figure \ref{fig:my_perlin} has been produced plotting in 3D the results of the function \texttt{pnoise2} from the library \texttt{noise}, which offers a tool for the production of different type of noise. In order to put in practical use this module some adjustment were required. The particular function \texttt{pnoise2} simply yields the value of the Perlin noise surface in correspondence to a single $(x,y)$ point in the plane in a deterministic way. There was not the possibility to generate different whole Perlin surfaces every run. To overcome this limitation I made a vectorized\footnote{In Python the staple tool for scientific, number-cruncher computation is the \texttt{NumPy} library, which allows a fast, complete and efficient way to perform computation between number structure. The operation of trasnforming a function which acts on a single value (or pair of values) to a function able to perform on a suitable datastructure is called \textit{vectorization}, and its the recomended way to proceed when handling numerical functions in Python.} version of \texttt{pnoise2} able to evaluate the function over an arbitrary wide grid of points expressed as set of all the pairs of coordinates $(x,y)$ of the grid's nodes. In this way a single call to the function is able to produce the entire surface covering the grid, in the form of a single \texttt{NumPy} 3D-array. Furthermore, to recover the possibility of generating always different surfaces as in a random generation, I inserted an offset cordinate $(x_O,y_O)$, which moves in the plane the origin of the surface generation. This pair of offset coordinates then acts also as a \textit{seed} in the generation, allowing to completely recreate previously generated material in a controlled way.

    \begin{figure}
        \centering
        \includegraphics[width = 0.6\textwidth]{images/perlin}
        \caption{Example of Perlin noise 2D funciton produced while working on this project. This surface offers a smooth variation around the value 0 with amplituded $ \in [ 0;1] $.}
        \label{fig:my_perlin}
    \end{figure}

    % [??]
    % citation to url - https://web.archive.org/web/20071011035810/http://noisemachine.com/talk1/

\section{Style-Transfer Neural Network} \label{ssec:sttrNN}
    Style-Transfer Neural Networks are common models, able of creating new hybrid images implanting the visual style from an image preserving the visual content of another image. The two images necessary for the algorithm are called \textit{style} image $S$ and \textit{content} image $C$, and the resulting \textit{styled} picture $X$, as in Figure \ref{fig:ex_st_tr}.

    There are many different tested and comparable architectures to compute this kind of algorithm. In my work I decided to use in particular the procedure described in \cite{1508.06576}, using the \texttt{PyTorch} ecosystem to implement the necessary code.

    \begin{figure}
        \centering
        \includegraphics[width = 0.9\textwidth]{images/st_trasf_ex}
        \caption{Different examples of application of a style-transfer NN on the same content image, with different style images, from \cite{1508.06576}. The
    original picture depicts the Neckarfront in TÃ¼bingen, Germany (TOP-LEFT). The painting used as style image shown in the bottom left corner of each panel are in clockwise order: $\bullet$ The Shipwreck of the Minotaur by J.M.W. Turner, 1805 $\bullet$ Femme nue assise by Pablo Picasso, 1910 $\bullet$ Composition VII by Wassily Kandinsky, 1913 $\bullet$ Der Schrei by Edvard Munch, 1893 $\bullet$ The Starry Night by Vincent van Gogh, 1889.}
        \label{fig:ex_st_tr}
    \end{figure}

    The backbone of the architecture is the VGG-19 network, which s a convolutional neural network 19 layers deep, as in Figure \ref{fig:vgg19}. This huge model has been pre-trained on over a million images from the ImageNet database \cite{imagenet_cvpr09}, for the classification into over than 1000 classes of objects. As a result, the network has learned rich feature representations for a wide range of images. The best (and conceptually the only) way to load a pre-trained model is to load the ordered set of weights that define the network and to initialize an empty module with those values. This is the perfect start for creating a style transfer network, which requires a further and briefer training phase, to completely customize the network.

    \begin{figure}
        \centering
        \includegraphics[width = 0.9\textwidth]{images/vgg19_str}
        \caption{The structure of the VGG 19 network. It is for its most a convolutional NN with $224 \times 224$ input size and with some downsampling layers which reduce the first two dimensions of the tensors along with the information flux of the network. At the very end of the architecture, there are three subsequent fully connected layers, responsible for the actual classification based on the features extracted from the previous layers.}
        \label{fig:vgg19}
    \end{figure}

    The key ingredient for finalizing the model is to insert some little but fundamental modifications and to extend the training on the pair of input images. This final training should be aware of the \textit{concepts} of the visual style and visual content of the image, and the operation should try to preserve them both. This is usually done minimizing two new loss function, computed between the staring image and the produced image:

    \begin{description}
        \item [Content Loss] \hfill \\
        The content loss is a function that represents a weighted version of the content distance for an individual layer.
        The most commonly used function to evaluate the preservation of content between two images is the simple Mean Squared Error as in equation (\ref{eq:MSE}). It can be computed between any couple of same-sized object, hence also between the results of the same feature maps on the images $X$ and $C$ at the same layer $L$:
        \begin{equation}
            L_{Cont} = \norm{F_{XL}âF_{CL}}^2.
            \label{eq:cont_loss}
        \end{equation}

        In order to evaluate this content loss, it is necessary to insert a custom transparent\footnote{A transparent layer is a layer that performs some operations, like evaluating a function on its input, but returns as output an unchanged copy of its input.} layer directly after the convolution layer(s) that are being used to compute the content distance.

        \item [Style Loss] \hfill \\
        The concept of \textit{style} loss function is the true novelty introduced by \cite{1508.06576}. This loss function is implemented similarly to the content loss module, as it will act as a transparent layer in the network. The computation of the style loss requires in advanced the evaluation of the Gram matrix $G_{XL}$ at a certain layer $L$.
        A Gram matrix is a result of multiplying a given matrix by its transposed matrix. In this case, the matrix to multiplicate is a reshaped version of the feature maps $F_{XL}$: $\hat{F}_{XL}$, a $K \times N$ matrix, where $K$ is the number of feature maps at layer $L$ and $N$ is the length of any vectorized feature map $F^k_{XL}$.
        Furthermore, the Gram matrix must be normalized by dividing each element by the total number of elements in the matrix. The style distance is now computed using the mean square error between $G_{XL}$ and $G_{SL}$:
        \begin{equation}
            L_{Style} = \norm{G_{XL}âG_{SL}}^2.
            \label{eq:styl_loss}
        \end{equation}
    \end{description}

    After the appropriate insertion of the loss-function evaluator layers, one last piece for finalizing the model is the right choice of the gradient descent optimizer. As in \cite{1508.06576} and according to the Deep Learning community the optimizer which suite best this role is the Limited Memory-BFGS \cite{10.1093/imamat/6.1.76},\cite{shanno1970conditioning}. L-BFGS is an iterative algorithm in the family of quasi-Newton methods that approximates the Broyden-Fletcher-Goldfarb-Shanno algorithm (BFGS) using a limited amount of computer memory, and it is a popular choice when estimating parameters of a non-linear differentiable scalar function.

    The final phase of the training process thus makes run the optimizer for hundreds of epochs on $X$, $C$, and $S$, and reduce the two loss functions values acting on the network's parameters. After the fine-tuning of the weights, the hybrid image is produced, as in Figure \ref{fig:ex_st_tr}.

\section{Working Environment} \label{ssec:my_machine}
In this section I will briefly desribe the machine I used to develop my project and the working environment I built.
All the work has been done on my personal computer, mounting a \texttt{GNU/linux} operating system, in particular 18.04 LTS \texttt{Ubuntu} version. The computer mounts an Intel i7 core, 8 Gb of RAM beside an 2Gb NVidia 940MX GPU.
All the Python libraries have been installed and harmonized in a virtual environment mounting \texttt{Python 3.7.6}.
All the code produced during the development, the images, and the data produced have been collected in a devoted repository on GitHub \cite{repo}, which is freely available.

\DIFaddbegin \DIFadd{As a conclusion for this chapter I will re-collect all the references to the different }\texttt{\DIFadd{Python}} \DIFadd{libraries I used during the development of this work:
}

\begin{description}
    \item [\texttt{\DIFadd{NumPy}}\DIFadd{:}] \texttt{\DIFadd{NumPy}} \DIFadd{is the pillar of every scientific computation-oriented library. Is the most spread library for heavy multidimensional numerical computation, and it offers a broad variety of tools like random numebr generators, and pre-implemented linear algebra utilities \mbox{%DIFAUXCMD
\cite{oliphant2006guide}}\hspace{0pt}%DIFAUXCMD
.
}

    \item [\texttt{\DIFadd{SciPy}}\DIFadd{:}] \DIFadd{The }\texttt{\DIFadd{SciPy}} \DIFadd{library is one of the core packages that make up the SciPy stack. It provides many user-friendly and efficient numerical routines, such as routines for numerical integration, interpolation, optimization, linear algebra, and statistics \mbox{%DIFAUXCMD
\cite{2020SciPy-NMeth}}\hspace{0pt}%DIFAUXCMD
. Two modules in particular form this libraries have covered an essential role in this project: the }\texttt{\DIFadd{SciPy.spatial.Voronoi}} \DIFadd{module for the computation of the 3D Voronoi decomposition, as mentioned in section \ref{ssec:vor_tass}, and the }\texttt{\DIFadd{SciPy.spatial.ConvexHull}} \DIFadd{module for the computation of 3D and 2D convex hulls (section \ref{ssec:pol_sec}).
}

    \item [\texttt{\DIFadd{PyTorch}}\DIFadd{:}] \texttt{\DIFadd{PyTorch}} \DIFadd{is a rich ecosystem of tools and libraries geared toward Machine Learning and Deep Learning. The application of the style-transfer NN described in section \ref{ssec:sttrNN} has required the use of this framework \mbox{%DIFAUXCMD
\cite{NEURIPS2019_9015}}\hspace{0pt}%DIFAUXCMD
.
}

    \item [\texttt{\DIFadd{SALib}}\DIFadd{:}] \DIFadd{The }\texttt{\DIFadd{SALib}} \DIFadd{is a library which collects many tools for the Sensitiviy Analysis of parameters. In particular the }\texttt{\DIFadd{SALib.saltelli}} \DIFadd{submodel was used for the quasi-random numerical sampling in a three-dimensional box, and it has been described in section \ref{ssec:saltelli}.
}

    \item [\texttt{\DIFadd{pnoise2}}\DIFadd{:}] \texttt{\DIFadd{pnoise2}} \DIFadd{contains many tools for the production of specific types of noise. The module }\texttt{\DIFadd{noise}} \DIFadd{was tweaked for the production of two-dimensional Perlin noise surfaces, and it has been introduced in section \ref{ssec:perlin}.
}

    \item [ \texttt{\DIFadd{pyquaternion}}\DIFadd{:}] \DIFadd{The }\texttt{\DIFadd{pyquaternion}} \DIFadd{library provides a framework for handling quaternions. It has been widely used in section \ref{ssec:quat} for the design of three-dimensional ramifications for handling multiple spatial rotations.
}\end{description}

        \DIFaddend \clearpage

    \chapter{Tissue Models Development} \label{sec:models}
        The main goal of the present work, as stated before, is to recreate a three-dimensional virtual model of histological tissue as faithfully as possible and then, to perform planar sectioning on it to emulate virtually the traditional histological specimen preparation procedure. The creation of a model of such complex structures is definitely a high-level problem, and it has required a careful designing, made of subsequent stages of improvements. 
In section \ref{ssec:panc_tis_mod}, I will describe all the necessary steps to create the model of a small region of pancreatic tissue, while in section \ref{ssec:derm_tis_mod} I will expose the steps I followed to build a model of dermal tissue. In section \ref{sec:synth_image}, instead, I will show the resulting synthetic images from the sectioning process performed on both the models and all the enrichments and processing necessary to give them the most realistic look I was able to recreate.

        \section{Pancreatic Tissue Model} \label{ssec:panc_tis_mod}
The Pancreas is an internal organ of the human body, part of both the digestive system and the endocrine system. It acts as a gland with both endocrine and exocrine functions, and it is located in the abdomen behind the stomach. Its main endocrine duty is the regulation of sugar levels in the blood and the secretion of hormones, like insulin and glucagon. While, as a part of the digestive system it acts as an exocrine gland secreting pancreatic juice. The majority of pancreatic tissue has a digestive role, and the cells with this role form clusters (\textit{acini}) around the small pancreatic ducts, and are arranged in lobes. The acinus secrete inactive digestive enzymes called zymogens into the small intercalated ducts which they surround, and then in the pancreatic blood vessels system \cite{Pancreas}. In Figure \ref{fig:panc_struct} is shown a picture of the pancreas, with its structure and its placement in the human body.

\begin{figure}
    \centering
    \includegraphics[width = 0.45\textwidth]{images/panc_struct}
    \includegraphics[width = 0.45\textwidth]{images/panc_specimen}
    \caption{(left) A picture of pancreas' structure in its phisiologiacl context. In this picture is clearly visible the macroscopic structure and the galndular organization at microscopic level, and how it reflects in the histological sample. (right) A real pancreatic tissue sample with H\&E staining.}
    \label{fig:panc_struct}
\end{figure}

All the tissue is actually rich in other important elements as the islets of Langerhans, and sporadic connective tissue all over the structure, which are clearly visible in the traditional histological specimens. In this first attempt of modelization from scratch this second layer of complexity has not been already considered, and the main focus has been to reflect only the main structural features on the virtual specimens. Given pancreatic tissue's organization the first features I decided to put emphasis on were: 1) The iterative (with a fractal-like behavior) ramification of blood vessels for the irrigation of glandular acinus, 2) The space-filling distribution of acinus in the tissue, in fact, we expect a homogeneous density in the organ and to not see \textit{holes} at all inside it. In this section I will describe step by step all the process I followed to create the model of a portion of pancreatic tissue, and all the interesting pitfalls I overcame.

% To make enumerated description:

\subsection{2D Ramification}
    The first step was took in two dimensions, and it was the choice of the right \textit{structure} to emulate the ramification of blood vessels in pancreatic tissue. The choice fell on a particular parametric L-system, as the one shown in Figure \ref{fig:bf_ls}, in section \ref{ssec:Lsys}. This structure is made of an iterative bifurcation of gradually shorter segments, with an angle of $\pm 85 \degree$ respect the main direction. For a start I added some features to give a more realistic look to the structure, which are all well represented in Figure \ref{fig:ram_feat}:

    \begin{figure}
        \centering
        \includegraphics[width = 0.6\textwidth]{images/ram_feat}
        \caption{The development of the simple ramification in Figure \ref{fig:bf_ls}, with some features to give it a more realistic look, like progressive thickness, angular noise in bifurcation and spheres at free ends of the ramification. The image is made using the tools exposed in section \ref{ssec:Lsys}.}
        \label{fig:ram_feat}
    \end{figure}

    \begin{itemize}
        \item A progressive thickness of the bifurcation's segments, starting from a thick main branch that dwindles every junction. The idea is that the main blood vessel becomes gradually smaller becoming capillaries for single-cell irrigation.
        \item A progressive randomness in the angular deflection at every fork. Perfectly repeated angles are almost nonexistent in nature, so I decided to introduce an increasing indetermination in the angle of bifurcation from the main branch to the free ends of the structures' branches.
        \item Spheres at the ends of each branch, which acts as glandular acini. The maximum radius is comparable to the length of the final segments.
        \item A mechanism to avoid self-superimposition between branches and spheres. After the insertion of noise, the cumulative effect on the final segments might lead different branches to intersect. This is clearly a paradoxical situation, as real tissues while growing naturally occupy the space in a gradual way.
    \end{itemize}

    To produce the specific image in Figure \ref{fig:ram_feat} I used a particular setting of the tool described in section \ref{ssec:Lsys}, which have a greatly wider range of customization and could be used to create many other different structures to the need.

\subsection{Expansion to 3D}
    The successive step I followed was to expand this structure in three dimensions and fill the space in each of the three directions. The idea to evolve the structure in Figure \ref{fig:ram_feat} is simply to twist of 90$\degree$ the ramification at every junction point, in such a way to exit the previous belonging plane. However, putting into practice this development has not been easy. The organization of the structure in a 3D space requires an appropriate system of reference for handling subsequent rotations in three dimensions. The best option for handling relative 3D rotations, often used in computer graphics and every kind of 3D modelization, are quaternions, as shown in section \ref{ssec:quat}.

    \begin{figure}
        \centering
        \includegraphics[width = 0.6\textwidth]{images/3d_ram}
        \caption{The three-dimensional expansion of the 2D ramification in Figure \ref{fig:ram_feat}.}
        \label{fig:3d_ram}
    \end{figure}

    In this new structure, segments are replaced with cylinders, and circles are replaced with spheres. At every bifurcation to every cylinder are applied the following transformations:
    \begin{itemize}
        \item a contraction in its extensions, regulated by an adjustable parameter $R$.
        \item the usual deviation of $\pm 85 \degree$ respect to the direction of the parent branch.
        \item a 90$\degree$ specific rotation along the axis of its parent branch.
    \end{itemize}

    The result of this procedure is a 3D ramification like the one in Figure \ref{fig:3d_ram}, in which we can recognize a good coverage of the space defined by the structure's boundaries and immediate relation with the 2D structure in Figure \ref{fig:ram_feat}. It should be noted that, in the further refinements of the model from now on, there won't be present the progressive angular indetermination on the direction of branches. Although it is a feature already implemented and working, it requires efficient control to avoid reciprocal overlapping between elements to produce a realistic structure. This second element has not been already developed and it would certainly enrich the representative power of the model.

    As for the 2D ramification the production of this structure has required the implementation of a tool for the 3D generation with a greatly wider power, able to produce almost any type of three-dimensional iterative structure after the right adjustment, and with a high degree of customization. It is necessary to mention the fundamental tool which allowed me to accomplish this step of the development, which is the \texttt{Python} library \texttt{VPython}: a library for 3D graphics visualization. This library allows a convenient and powerful interface to draw many types of objects and to move them around in space, which has been priceless to orient my self in three dimensions while developing the model and to produce all the 3D images visible in this work.

\subsection{Subdivision in Cells}
    Once the 3D backbone of the pancreatic tissue blood vessels ramification system has taken shape, the next step was to embed all this structure in a spatial partitioning process, to create the subdivision into single cells. To perform this important task I used a 3D Voronoi decomposition, as shown in section \ref{ssec:vor_tass}. Depending on the choice of the starting points, the Voronoi tessellation could be an excellent item to recreate individual cells because it could guarantee some important properties: all the regions are convex, adjacent, with similar size and volume, with different shapes, and without holes. These have been chosen as the most significant properties to be reflected in the first modelization of cells.

    As shown in section \ref{ssec:vor_tass}, the Voronoi decomposition strongly depends on the choice of the starting point. Points spread uniformly on a 3D regular lattice will produce a series of parallelepipeds repeated in the space. An example of uniform tessellation is shown in Figure \ref{fig:reg_vor}. On the other hand, a decomposition based on a quasi-random generated point can present all the good properties we mentioned before, including the diversity in shapes. In Figure \ref{fig:sal_vor} is shown an example of a Voronoi decomposition based on points sampled in a 3D with the Saltelli algorithm, in reference to section \ref{ssec:saltelli}. Regardless of the points sampling technique, the boundaries of the sampling 3D box have been chosen to loosely contain the ramification.

    \begin{figure}
        \centering
        \begin{subfigure}[b]{0.45\textwidth}
             \centering
             \includegraphics[width = \textwidth]{images/reg_vor}
             \caption{Regular lattice.}
             \label{fig:reg_vor}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.45\textwidth}
             \centering
             \includegraphics[width = \textwidth]{images/sal_vor}
             \caption{Sampling with Saltelli algorithm.}
             \label{fig:sal_vor}
        \end{subfigure}
        \caption{Comparison between two Voronoi decompositions. The first (left) is created from a regular lattice of starting points, and every piece is exactly equal to all the others, creating a regular subdivision of the space. The second (right) is created instead from a sampling made following the Saltelli quasi-random algorithm. The pieces are all different in shape, but they all have similar sizes and volumes. In this pictures in particular have been shown only the pieces of the tessellation which lie in correspondence to the boundaries of the spheres underneath. While watching this picture one should immagine the decomposition extended similarly in all the space around the ramification, within certain boundaries, which loosely contains the structure. This limitation was necessary to enhance the interpretability of the image. }
        \label{fig:vor_comp}
    \end{figure}

    There are tough some delicate considerations to be highlighted about the decomposition procedure. The first regards the most external pieces of the decomposition. Whilst the internal pieces are neatly bounded and defined, the most external layer instead is made on unbounded regions, which extend themselves to infinity. Those regions have clearly to be rejected, as it would be absurd for a cell to have an infinity volume. Typically those unbounded regions are resized in order to adhere to some limiting boundaries, with an operation known as \textit{cropping}. In Figure \ref{fig:crop_vor} is shown an example of circular cropping in a 2D Voronoi decomposition: all the regions which intersect the circumference have to be resized.

    \begin{figure}
        \centering
        \includegraphics[width = 0.5\textwidth]{images/crop_vor}
        \caption{Example of circular cropping in a 2D Voronoi decomposition: all the regions which intersect the circumference have to be resized.}
        \label{fig:crop_vor}
    \end{figure}

    The cropping operation in 3D is extremely complex, tough. Thus, a more simple and efficient, yet less elegant, technique has been used. Instead of resizing the regions which lie on the boundaries of the sampling region, those regions have directly been rejected. This process is really fast and it does not lead to any danger of representativity loss if the boundaries are loose enough and if the density of sampling is not too low.

    The other important consideration regards the density of sampling points. Increasing the number of points to be extracted from the same volume automatically the number of cells in the box will rise, and in contrast, their relative dimension will decrease. This is a key element of the model: a too rarified decomposition would not bee able to reflect the complexity of the structure underneath, but a too crowd decomposition on the other side would lead to an unrealistic dimension of the cells in the tissue. Furthermore, this parameter has a huge influence on the computing time necessary to generate the model and to process it for the sectioning as will be shown in section \ref{sec:synth_image}. In almost all the applications so far, the density parameter has been tuned by eye, with a trial and error procedure. Although, a more rigorous way to adjust this parameter would be to consider the average dimension of the cells and make some microanatomical considerations to define the correct relative dimensions. The measure of the volume\footnote{The volume is expressed in the same arbitrary length unit of measures used during the ramification structure. This allows a coherent reference tool.} of the decomposition's regions is an accessible parameter, thus an easy way to estimate the average linear dimension of the cells can be to approximate all the cells to cuboid seeing their volumes as $V \approx L^3$. Averaging all the $L$ measures an estimate $\hat{L}$ can be done. This average length may be compared to the length of the blood vessel ramification, allowing a good reference tool.

\subsection{Cells Identity Assignment}
    The great power of creating all the models virtually is to know exactly the identity of every point in the structure. Although, This identity has to be reflected at the cellular level, assigning to every region a label. Imagining the Voronoi decomposition represented in Figure  \ref{fig:sal_vor} extended to the entire box containing the ramification, good discrimination would distinguish three classes of cells: those which lie completely inside a sphere, those which lie completely outside a sphere, and those which lie on the boundaries of a sphere. In Figure \ref{fig:cell_id} is shown a portion of the complete decomposition where the three classes of cells are reported with different colors: the internal cells in red, those on the boundaries in turquoise, and the external cells in gray.

    \begin{figure}
        \centering
        \includegraphics[width = 0.5\textwidth]{images/cell_id}
        \caption{Portion of the complete Voronoi decomposition, showing the three different classes of cell in three different colors: the internal cells in red, those on the boundaries in turquoise and the external cells in gray.}
        \label{fig:cell_id}
    \end{figure}

    In this particular case to find the relative position between every sphere in the structure and each cell it has been used a test on the proximity between the spheres' centers and the vertices of every polyhedral cell. If all the vertices of a region lie within a distance lower than the radius from the center of the same sphere then that region can be said to be an internal one. If none of the vertices lie within the radius distance from any center then that region is said to be external. In any other case, the region is said to be on the boundaries of some sphere, and this third label is assigned to it. As could be imagined the number of cells inside the volume can grow very quickly, and in the more rich ramifications also the number of spheres could be high. If we think that any polyhedron has a number of vertices of the order of $20/30$ then it is clear that the number of distance evaluations could grow very quickly, requiring some relevant computational power in the more extended simulations. In order to optimize this computation, I decided to use a python implementation of a K-dimensional Tree, which is a space-partitioning data structure especially suited for fast and optimized computation of distances \cite{10.1145/361002.361007}. A K-d Tree is an algorithm that iteratively binary splits the space: every node of the three could be thought as a splitting $(k-1)$-hyperplane dividing the space into two semi-hyperspace. The result is an optimized algorithm for repeated distance evaluations. As for many other tools, in my code I used a pre-implemented module \texttt{KDTree} from the \texttt{Scipy} library.

    This procedure of labeling the regions is completely customizable, and it should be adapted to the specific application. By the way, the principle will always be to perform some sort of spatial consideration respect to the primary structure and assign all the interesting labels accordingly to the cells in the volume.

    After labeling the cells in the decomposition the model is considered complete. Every enrichment to the structure should be reflected in some type of label for the cells, which are chosen as the fundamental unit in the model. As we will see in section \ref{sec:synth_image} during the sectioning process in the produced image will be printed mainly the identity of the cells, hence any detail on a finer scale in the model would not be conveyed properly on the final image.

\section{Dermal Tissue Model} \label{ssec:derm_tis_mod}
\hl{\textit{The dermis or corium is a layer of skin between the epidermis (with which it makes up the cutis) and subcutaneous tissues, that primarily consists of dense irregular connective tissue and cushions the body from stress and strain. It is divided into two layers, the superficial area adjacent to the epidermis called the papillary region and a deep thicker area known as the reticular dermis.[1] The dermis is tightly connected to the epidermis through a basement membrane. Structural components of the dermis are collagen, elastic fibers, and extrafibrillar matrix.[2] It also contains mechanoreceptors that provide the sense of touch and thermoreceptors that provide the sense of heat. In addition, hair follicles, sweat glands, sebaceous glands (oil glands), apocrine glands, lymphatic vessels, nerves and blood vessels are present in the dermis. Those blood vessels provide nourishment and waste removal for both dermal and epidermal cells.
}}
    \begin{figure}
        \centering
        \begin{subfigure}[b]{0.45\textwidth}
             \centering
             \includegraphics[width = \textwidth]{images/derm_scheme}
             \caption{}
             \label{fig:derm_scheme}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.45\textwidth}
             \centering
             \includegraphics[width = \textwidth]{images/derm_specimen}
             \caption{}
             \label{fig:derm_specimen}
        \end{subfigure}
        \caption{(left) Microanatomical description of a region of dermal tissue and all the interesting elements present in cutis, and subcutaneous layer. (right) An actual histological specimen from a sample of dermal tissue.}
        \label{fig:derm_descr}
    \end{figure}

The modeling of the dermal tissue has followed analogous schedule respect to the previous model, hence many procedures and considerations have been repeated. The main target for this model was to recreate the stratification of different specific tissues in the section of a dermal sample. As clearly visible in Figure \ref{fig:derm_specimen} in a dermal histological specimen one can distinguish the lighter and wider region of proper \textit{dermal} tissue, underneath a more shallow and darker region of \textit{epidermal} tissue. It is very interesting the boundary between those two regions, which can be seen as an irregular and smooth surface, populated of dermal lobes. On the other side of the epidermal layer lies a layer of \textit{keratin}, with a smooth and regular boundary surface. Keratin is a family of fibrous structural proteins known as scleroproteins, a key structural material for hair, nails, and the outer layer of skin. The upper white region in Figure \ref{fig:derm_specimen} instead is the support for the samples to perform optical analysis, and has no histological meaning.

\begin{description}
    \item [1) Stratificated Structure] \hfill \\
    In order to represent faithfully the stratification of different tissue layers, I decided to use one flat plane to separate epidermal tissue and the keratine layer and two boundary surfaces modulated by a Perlin noise function on different scales for the other two separations, as shown in Figure \ref{fig:2_surf_plot}. As stated in section \ref{ssec:perlin}, after some easy customization the generation of different Perlin noise surfaces is easy and straightforward. By the way, to achieve the regularity and the smoothness of the orange surface in Figure \ref{fig:2_surf_plot} it was needed a horizontal stretching.

    \begin{figure}
        \centering
        \includegraphics[width = 0.6\textwidth]{images/2_surf_plot}
        \caption{Pictures of two different Perlin noise surfaces used to separate dermal from epidermal tissue (blue) and epidermal from keratine layer (orange). The two surface are made by the same Perlin noise function, but the latter is stretched and compressed in order to have a more regular behavior.}
        \label{fig:2_surf_plot}
    \end{figure}

    Following the scale of the image, the standard blue surface is created in a $7 \times 7$ square, while the orange surface has primarily been generated in a $1 \times 1$ square, and then has been stretched to cover the same $7 \times 7$ square, multiplying the values in its grid points respectively for the stretching factors $R_x=R_y=\frac{1}{7}$. In this primary structure, there are many important parameters defining the surfaces, like the distance between the two surfaes' average values and the amplitudes of the peaks and the valleys of the surfaces. In its standard version the Perlin noise covers the $[-1;1]$ range, but with a simple multiplication for an amplitude factor $A_S$ the values can be adjusted. Those particular values have been adjusted after some tries to recreate the proportions typical of a real specimen. To sum up, each one of the two surfaces is stored as a discretized three-dimensional array, or better as an array of 3-tuples in the form $(x,y,f_{(x,y)})$, one tuple for every $(x,y)$ node of the grid, while the discretization grid was the same for both the surfaces.

    \item [2) Subdivison in Cells] \hfill \\
    The subdivision in cells of the volume containing the structure has followed the exact same steps described in section \ref{ssec:panc_tis_mod}, hence in this paragraph I will shortly resume the process. The first step is the definition of a suitable volume containing the structure. Then is the time for the generation of the decomposition's starting points according to a quasi-random number generation technique, as described in section \ref{ssec:saltelli}. Afterward, the points are used as a base for the decomposition, and all the cells with undefined boundaries are rejected

    \item [3) Cells Identity Assignment] \hfill \\
    The identity assignment procedure instead has been customized for this particular application. In this model there are no \textit{boundary} cells as the one lying on the spherical surfaces in the pancratic model, thus there is no need for a test on the position of each vertex of every cell. In this model, the starting point for every Voronoi region has been used as a reference, and its relative position respect to the boundary surfaces was the discriminating factor for assessing the identity. In order to perform a coherent test on the relative position between the regions' center and the boundaries surfaces the positions of all the centers had to be discretized on the same grid onto which were defined the surfaces. The comparison with the flat horizontal plane defining the boundary between epidermal tissue and the keratine layer instead was simply a test on the $z$ coordinate of the point: $z = f_{(x,y)} \leq \hat{z}_{plane}$. The result of this procedure is that every region is assigned a label corresponding to the belonging tissue layer: $D$ for dermal, $E$ for epidermal, $K$ for keratine, and $V$, which stands for \textit{void} for the white empty space above the sample.

\end{description}

In this model, as in section \ref{ssec:panc_tis_mod}, after the assignment of the identity to all the cells in the volume the modelization process is considered complete. Any sort of improvement and enrichment should be inserted during the structure designing phase and should be linked to an identity label.

        \clearpage
        \section{Synthetic Images Production} \label{sec:synth_image}
    After the model is complete we have a three-dimensional representation of the tissue under study. The aim of the work is though to produce synthetic images from that structure, more precisely we want pairs of images composed of the synthetic-histological images and its related segmentation mask. The transition from 3D structure to a 2D image is the last step in the process, and it is inspired by the actual traditional technique for the preparation of the histological specimen, as described back in section \ref{ssec:samp_prep}. As the biopsy sample is treated and then sectioned with the microtome, the virtual model is sectioned in a random direction, producing an image representing the slice. This first image contains all the information of the section but its appearance is completely arbitrary and its look has nothing to share with a realistic sample. The original slice then acts perfectly as a segmentation mask, but some careful and dramatic makeover is needed to produce the final realistic looking image. In this section I will describe the general procedure to produce virtual slices from the two 3D virtual models described before in section \ref{sec:models} and the technique used to edit the images and give them the desired appearance.

\subsection{Sectioning Process} \label{ssec:sect_proc}
    For any model created following the general procedure described in \ref{sec:models}, even more so for the two particular models of pancreatic and dermal tissue, the sectioning process will be almost the same, and it will rely mainly on the algorithm for the general section of polyhedron described back in section \ref{ssec:pol_sec}. As stated before the models are essentially composed by labeled polyhedron spatially organized in a 3D volume. The ordered section of each polyhedron will yield all the polygons that shall be assembled in the final section.
    In Figure \ref{fig:sec_3D} is shown the three-dimensional representation of the section of a simple ramification, as the one in Figure \ref{fig:cell_id}. All the polygons that compose the section are drawn with the color correspondent to their label, following the same idea of Figure \ref{fig:cell_id}. In Figure \ref{fig:section_simple} is printed the final result of the sectioning algorithm applied to the model, which will be the segmentation mask in the single pair of synthetic images. The colors in the produced slice match the colors used for the different identities in the 3D model.

        \begin{figure}[ht]
            \centering
            \begin{subfigure}[t]{0.45\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/sec_3D}
                 \caption{2D polygonal sections represented in a 3D environment.}
                 \label{fig:sec_3D}
            \end{subfigure}
            \hfill
            \begin{subfigure}[t]{0.45\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/section_simple}
                 \caption{The fianl section image, produced directly on a planar picture, skipping completely the 3D representation.}
                 \label{fig:section_simple}
            \end{subfigure}
            \caption{In this Figure is shown the idea of the correspondence between the section of a simple structure in the space and the correspondent section. The correspondence is not perfect for representation requirments, in fact the two images even if very similar are produced with two completely different methods. At the left an image of 2D polygonal section embeded in a 3D space, made using 3D visualization tools. At the right a simple image produced printing the polygons in a planar picture. Printing 2D polygons in a 3D space is much more complicated than one would think using the same tool used to produce the other images like Figure \ref{fig:cell_id} and \ref{fig:vor_comp}. This choice has been done for the sake of the overall homogeneity in pictures style.}
            \label{fig:first_sect}
        \end{figure}

    The simplest, yet over-abundant, way to proceed is to create the model in its entirety and subsequently choosing the sectioning plane. Afterward, it is necessary to select only the regions that intersect the plane and section them all. Actually, the test on the intersection passes through the check on the relative position of the polyhedron's vertices respect to the sectioning plane: if all the vertices lie on the same semi-space then the intersection would be null and the polyhedron is not of interest for that particular section. This procedure is exactly the first step of the algorithm in \ref{ssec:pol_sec}, thus the filtering on the regions is actually made during construction for optimization. The alternative method could be to choose in the first place the direction of the sectioning plane, and in second place to generate the model's decomposition only in the volume adjacent to that plane. This method enables the sparing of a good amount of computation, without any negative impact on the final result. The only delicate step is the choice of a wide enough region of space around the plane, which doesn't compromise the representation.

    As a guarantee for richness and diversification among the images, there is the need for some degree of controlled randomness in the sectioning process, for example in the determination of the sectioning plane direction. All the sectioning process is then based on a single starting \textit{seed}, which determines the direction of the sectioning plane in a deterministic way, and all the rest of the model is generated as a consequence. In this way, all the possible angulations are equally probable and will be sampled in view of multiple applications of this process. In Figure \ref{fig:4sph_sections} are shown two different sections, along two random planes on two simple ramified structures with four spheres.

        \begin{figure}
            \centering
            \begin{subfigure}[t]{0.2\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/4sph_sec1}
                 \label{fig:4sph_sec1}
            \end{subfigure}
            \quad
            \begin{subfigure}[t]{0.2\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/4sph_sec2}
                 \label{fig:4sph_sec2}
            \end{subfigure}
            \caption{Two different section, along two randon planes on two simple ramificated structure with four spheres.}
            \label{fig:4sph_sections}
        \end{figure}

\subsection{Appearance Makeover} \label{ssec:app_mkov}
    After the application of the sectioning algorithm of the previous section, the image which will act as a segmentation mask is ready. The last and most complex task that remains is to transform the image and to give it a realistic look. I tried many different transformations, more or less complicated, and there was not a final decision on which is the best blend of them. In this section, I will describe them as an arsenal of possibilities and show their impact on the images.

    \begin{description}
        \item [Color Palette] \hfill \\
        The first correction to do to the images will inevitably be a change in the colors of the image. Gray, Turquoise, and Red are the perfect choice for label-colors but act poorly as physiological colors. In Figure \ref{fig:new_palette} is shown an example of an image produced re-mapping the colors to a new palette, inspired to the coloring of the real specimen in Figure \ref{fig:panc_struct} and \ref{fig:he_retyna}, given by the traditional hematoxylin and eosin staining process.

        \begin{figure}[h]
            \centering
            \includegraphics[width = 0.2\textwidth]{images/new_palette}
            \caption{Example of images produced re-mapping the colors with a color palette inspired to a real H\&E stained histological sample.}
            \label{fig:new_palette}
        \end{figure}

        \item [Nuclei Projection] \hfill \\
        Another fundamental processing needed was the projection of cells' nuclei on the image. Usually, nuclei are clearly visible in histological samples and guide the analysis allowing to detect individual cells in the specimen. As a reference for the nucleus position the starting point of every polyhedral region has been used and projected on the sectioning plane as a little dark circle. The diameter of those circles as been chosen to be a submultiple (10\%) of the linear estimated dimension $\hat{L}$ of the cells in the decomposition\footnote{Following the same logic of step 3 of section \ref{ssec:panc_tis_mod}.}.

        \begin{figure}
            \centering
            \begin{subfigure}[t]{0.2\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/nuclei_mask}
                 \label{fig:nuclei_mask}
            \end{subfigure}
            \quad
            \begin{subfigure}[t]{0.2\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/nuclei_real}
                 \label{fig:nuclei_real}
            \end{subfigure}
            \caption{Nuclei projection on the image: (left) in yellow in the segmentation mask and (right) in purple in the image under makeover.}
            \label{fig:nuclei_proj}
        \end{figure}

        Nuclei projection, among the other things, is an excellent tool to perceive the different effects obtained with different choices of quasi-random distributions or fully-random distributions (with reference to section \ref{ssec:saltelli}). The different impact on the overall image is huge, and it really changes the overall sense of the image. In Figure \ref{fig:sampling_comparison} are reported four different sections, produced with the same density on cells but with four different methods for the sampling of starting decomposition's points.

        \begin{figure}
            \centering
            \includegraphics[width =0.24 \textwidth]{images/lattice}
            \includegraphics[width =0.24 \textwidth]{images/sequence}
            \includegraphics[width =0.24 \textwidth]{images/saltelli}
            \includegraphics[width =0.24 \textwidth]{images/uniform}
            \caption{Four different sections produced with the same density on cells but with four different method for the sampling of starting decomposition's ponits (from left to right): $\bullet$ points sampled on a regular lattice, $\bullet$ sampling following a simple recursion sequence as the one in equation (\ref{eq:ad_rec}), $\bullet$ following the \textit{saltelli} algorithm, $\bullet$ following a fully-random distribution.}
            \label{fig:sampling_comparison}
        \end{figure}

        \item [Boundaries Projection] \hfill \\
        On the same wave of the previous tool, another operation that can help the appearance of an image is the projection of the boundaries of each (or just a part) of the polygonal sections. The drawing can be clearly tuned and customized depending on the specific necessities. In Figure \ref{fig:derma_slice} is shown an example of a section on the dermal tissue model in \ref{ssec:derm_tis_mod}, in which the boundaries of all the cells have been lightly marked, and the boundaries of the cells in the keratine layer have been heavily marked instead.

        \begin{figure}
            \centering
            \includegraphics[width = 0.4\textwidth]{images/derma_slice}
            \caption{Example of images produced re-mapping the colors with a color palette inspired to a real H\&E stained histological sample.}
            \label{fig:derma_slice}
        \end{figure}

        \item [Blurring Effects] \hfill \\
        In all the images produced so far the boundaries between polygonal sections are perfectly sharp and without any smudge. To give a more realistic feeling to those pictures I tried different forms of blurring. As a first try, I applied a Gaussian blurring filter, which is an extremely common blurring operation in computer vision, which consists of a simple discrete convolution with a 2D Gaussian kernel. The effect is a regular and diffuse blur all over the image, as in Figure \ref{fig:gauss_blurr}. The second blurring effect I implemented was based instead on the averaging of parallel and adjacent slices on the same model. This method is inspired by the real sectioning technique (section \ref{ssec:samp_prep}), in which every slice is not an infinitesimal layer of matter, but a finite sample, which suffers from mechanical dragging during the process. The idea is that the average between three or more slices equally spaced above and below the \textit{main} slice should recreate a realistic blurring effect. An example of an image produced with this process is shown in Figure \ref{fig:av_blur}.

        \begin{figure}
            \centering
            \begin{subfigure}[t]{0.3\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/gauss_blurr}
                 \caption{}
                 \label{fig:gauss_blurr}
            \end{subfigure}
            \quad
            \begin{subfigure}[t]{0.3\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/average}
                 \caption{}
                 \label{fig:av_blur}
            \end{subfigure}
            \quad
            \begin{subfigure}[t]{0.3\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/rgb_prlin}
                 \caption{}
                 \label{fig:rgb_prlin}
            \end{subfigure}

            \caption{The two blurring effects used in this work: (left) A standard Gaussian-filter blur and (center) a specific blur introduced averaging adjacent parallel slices on the same image, (right) an example of RGB color noise built joining three different Perlin noise surfaces, one for each color channel.}
            \label{fig:blur_effect}
        \end{figure}

        \item [Perlin RGB Noise] \hfill \\
        A further attempt to give visual texture to the image was done using again the Perlin noise, described in section \ref{ssec:perlin}. The idea is to create some fluctuation among the color channels of the image around the sharp values of the image produced by the sectioning algorithm.
        From a practical point of view, I created three different and independent Perlin noise surfaces, one for every color channel (Red, Green, and Blue), and added them to create an RGB noise on the image. An example of the resulting image is shown in Figure \ref{fig:rgb_prlin}.

        \item [Style Transfer] \hfill \\
        This last tool I will describe is the most sophisticated so far. It consists of the application of a style-transfer neural network (STNN) on the image obtained through the sectioning process, for the implantation of the visual texture from a real sample of the corresponding tissue. Style-transfer NNs, and their functioning, have been described in detail in section \ref{ssec:sttrNN}, and here I will cover just the particular applications on the two type of section produced.

        The first manipulation I report is the one on a section from the pancreatic tissue model. The image of which to conserve the visual content is a section with some simple pre-processing picked from the ones described before (Figure \ref{fig:st_nn1}): a more accurate color palette, the projection of nuclei, and the average on five adjacent slices. The image from which to pick the style, thus the visual texture, is a portion of an actual histological sample of the pancreas, and it is shown in Figure \ref{fig:st_nn2}. The application of the STNN yields a hybrid image, shown in Figure \ref{fig:st_nn3}. The second application was made on a section on dermal tissue. The three content, style and styled images are shown respectively in Figure \ref{fig:st_nn4}, \ref{fig:st_nn5}, and \ref{fig:st_nn6}.

        \begin{figure}
            \centering
            \begin{subfigure}[t]{0.3\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/st_nn1}
                 \caption{}
                 \label{fig:st_nn1}
            \end{subfigure}
            \quad
            \begin{subfigure}[t]{0.3\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/st_nn2}
                 \caption{}
                 \label{fig:st_nn2}
            \end{subfigure}
            \quad
            \begin{subfigure}[t]{0.3\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/st_nn3}
                 \caption{}
                 \label{fig:st_nn3}
            \end{subfigure}
            \caption{Application of the style-transfer NN on a section of the pancreatic tissue model: \ref{fig:st_nn1} the content image, \ref{fig:st_nn2} the style image, \ref{fig:st_nn3} the hybrid resulting image.}
            \label{fig:panc_stnn}
        \end{figure}

        In Figures \ref{fig:panc_stnn}, and \ref{fig:derm_stnn} are reported the best results among all the different tests made on the sections. I made different tries on the same image with different processing before the manipulation with the STNN, to see the impact of the different adjustment on the resulting styled image. It turned out that the presence of nuclei is essential to give a homogeneous texture to the image and avoid unrealistic artifacts. On the other hand, the choice of the color palette has a way lighter effet than what one would think: the model yields almost the same result with a grey-levels image or with any other palette.

        It interesting to notice the timing cost of this style transfer operation. While all the other manipulation described in this chapter requires a very short time (seconds) to be applied, and are in practice \textit{instantaneous}, the transfer of style is a way more robust operation, which implies the finalization of the training of a pre-trained neural network. On a computer with the technical specification described in section \ref{ssec:my_machine} this operation instead took minutes, which is a time two full orders of magnitude greater.

        \begin{figure}
            \centering
            \begin{subfigure}[t]{0.3\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/st_nn4}
                 \caption{}
                 \label{fig:st_nn4}
            \end{subfigure}
            \quad
            \begin{subfigure}[t]{0.3\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/st_nn5}
                 \caption{}
                 \label{fig:st_nn5}
            \end{subfigure}
            \quad
            \begin{subfigure}[t]{0.3\textwidth}
                 \centering
                 \includegraphics[width = \textwidth]{images/st_nn6}
                 \caption{}
                 \label{fig:st_nn6}
            \end{subfigure}
            \caption{Application of the style-transfer NN on a section of the dermal tissue model: \ref{fig:st_nn4} the content image, \ref{fig:st_nn5} the style image, \ref{fig:st_nn6} the hybrid resulting image.}
            \label{fig:derm_stnn}
        \end{figure}

        It should be noted that the presented results are obtained from the application of a pre-trained STNN model. The development of a specialized model for histological texture transfer could improve extremely the ability to produce realistic images, way further the present results.
    \end{description}

One single complete application of the process consists then in the generation of a tissue model, in the sectioning along a random section plane, and in the processing of the image, in order to produce the pair of ground-truth image and the synthetic histological image. The target is to apply over and over this process to collect the necessary amount of images and constitute an entire dataset. An important feature to have for the process is thus a complete automatization, in order to scale up the generation of images, possibly even in parallel computation.
For this purpose, I created a pipeline workflow interface for the image generation, with an automatized harmonization of every piece of the process. The generation now requires just to fill a configuration file in which writes all the specific characteristics of the images: the type of structure, its features, the desired processing on the images, and eventually the random seeds for a supervised generation. In Figure \ref{fig:dataset} is reported a small scale example of a dataset produced with multiple automatized applications of the generation tool on a ramified structure inspired to a pancreatic tissue model. It is clear the correspondence between segmentation mask and synthetic histological images, and the diversification given by the supervised randomness on the generation.

The actual tool used for the set up of a working pipeline was the \texttt{Snakemake} \cite{10.1093/bioinformatics/bts480} workflow management system, which is a python-based tool to create reproducible and scalable data analyses.

    \begin{figure}
        \centering
        \includegraphics[height=20cm,keepaspectratio]{images/dataset}
        \caption{Small scale example of dataset produced with multiple automatized applications of the generation tool on a ramificated structure inspired to a pancreatic tissue model.}
        \label{fig:dataset}
    \DIFdelbeginFL %DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\subsection{\DIFdel{Alternative Works on Synthetic Histological Images Generation}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdel{As a conclusion to this chapter, I want to make an overview of some other interesting works on the generation of synthetic histological images, which have followed completely different paths and strategies.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The first work I want to cite is a work from Ben Cheikh }\textit{\DIFdel{et al.}} %DIFAUXCMD
\DIFdel{from 2017 \mbox{%DIFAUXCMD
\cite{10.1117/12.2254452}}\hspace{0pt}%DIFAUXCMD
. In this work, they present a methodology for the generation of synthetic images of different types of breast carcinomas. They propose a method completely based on two-dimensional morphology operation, as successive image dilations and erosions. With the modulation of a very restricted number of parameters, regulating the abundance of the objects, their distribution in the image, and their shapes they are able to reconstruct realistic images reflecting different histological situations. In Figure \ref{fig:morpho_tripl} is shown an example of generated material besides a real histological H\&E stained sample. Starting from a generated segmentation mask which defines the }\textit{\DIFdel{tumoral pattern}} %DIFAUXCMD
\DIFdel{the production of the synthetic image passes through successive steps, as the generation of characteristic collagen fibers around the structure, the injection of all the immune system cells, and some general
final refinements.
}%DIFDELCMD < 

%DIFDELCMD <     \begin{figure}[ht]
%DIFDELCMD <         \centering
%DIFDELCMD <         \begin{subfigure}[t]{0.3\textwidth}
%DIFDELCMD <              \centering
%DIFDELCMD <              \includegraphics[width = \textwidth]{images/morpho_mask}
%DIFDELCMD <              %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
}
             %DIFAUXCMD
%DIFDELCMD < \label{fig:morpho_mask}
%DIFDELCMD <         \end{subfigure}
%DIFDELCMD <         \quad
%DIFDELCMD <         \begin{subfigure}[t]{0.3\textwidth}
%DIFDELCMD <              \centering
%DIFDELCMD <              \includegraphics[width = \textwidth]{images/morpho_model}
%DIFDELCMD <              %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
}
             %DIFAUXCMD
%DIFDELCMD < \label{fig:morpho_model}
%DIFDELCMD <         \end{subfigure}
%DIFDELCMD <         \quad
%DIFDELCMD <         \begin{subfigure}[t]{0.3\textwidth}
%DIFDELCMD <              \centering
%DIFDELCMD <              \includegraphics[width = \textwidth]{images/morpho_real}
%DIFDELCMD <              %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
}
             %DIFAUXCMD
%DIFDELCMD < \label{fig:morpho_real}
%DIFDELCMD <         \end{subfigure}
%DIFDELCMD <         %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Example of generated tumoral pattern (left), which acts as segmentation mask, of generated image (center) and a real example of the tissue to recreate. From \mbox{%DIFAUXCMD
\cite{10.1117/12.2254452}}\hspace{0pt}%DIFAUXCMD
.}}
        %DIFAUXCMD
%DIFDELCMD < \label{fig:morpho_tripl}
%DIFDELCMD <     \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  \hl{Comment to adjust}
%DIF <  Although this strategy produces realistic-looking samples, the generation of the starting tumoral pattern is ruled by the random sampling of sparse seed points, the distribution of which should be finely tuned for the production of an accurate representation. Furthermore, this procedure is based completely on the phenomenological replication of the image, without any try to capture the spatial organization of an actual tissue ....
%DIF <  \hl{Comment to adjust}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The second work I want to mention is base on a DL-base technique, which approaches synthetic image generation using a specific cGAN architecture inspired to the ``U-net'' \mbox{%DIFAUXCMD
\cite{Senaras2018} }\hspace{0pt}%DIFAUXCMD
model described back in Figure \ref{fig:unet} in section \ref{ssec:soa_seg}.
This model works with Ki67 stained samples of breast cancer tissue, and it is able to generate high-fidelity images starting from a given segmentation mask. Those starting segmentation masks tough are obtained through the processing of other real histological samples, via a nuclei-detection algorithm. The differences between real and synthetic samples are imperceptible, and the material generated in this work has effectively fooled experts, who qualified it has indistinguishable from the real one. In Figure \ref{fig:cgan_tripl} an example of a real image, a generated one, and their corresponding segmentation mask.
}%DIFDELCMD < 

%DIFDELCMD <     \begin{figure}[t]
%DIFDELCMD <         \centering
%DIFDELCMD <         \begin{subfigure}[t]{0.3\textwidth}
%DIFDELCMD <              \centering
%DIFDELCMD <              \includegraphics[width = \textwidth]{images/cgan_mask}
%DIFDELCMD <              %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
}
             %DIFAUXCMD
%DIFDELCMD < \label{fig:cgan_mask}
%DIFDELCMD <         \end{subfigure}
%DIFDELCMD <         \quad
%DIFDELCMD <         \begin{subfigure}[t]{0.3\textwidth}
%DIFDELCMD <              \centering
%DIFDELCMD <              \includegraphics[width = \textwidth]{images/cgan_model}
%DIFDELCMD <              %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
}
             %DIFAUXCMD
%DIFDELCMD < \label{fig:cgan_model}
%DIFDELCMD <         \end{subfigure}
%DIFDELCMD <         \quad
%DIFDELCMD <         \begin{subfigure}[t]{0.3\textwidth}
%DIFDELCMD <              \centering
%DIFDELCMD <              \includegraphics[width = \textwidth]{images/cgan_real}
%DIFDELCMD <              %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
}
             %DIFAUXCMD
%DIFDELCMD < \label{fig:cgan_real}
%DIFDELCMD <         \end{subfigure}
%DIFDELCMD <         %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Example of generated tumoral pattern (left), which acts as segmentation mask, of generated image (center) and a real example of the tissue to recreate. From \mbox{%DIFAUXCMD
\cite{Senaras2018}}\hspace{0pt}%DIFAUXCMD
.}}
        %DIFAUXCMD
%DIFDELCMD < \label{fig:cgan_tripl}
%DIFDELCMD <     %%%
\DIFdelendFL \end{figure}

        \clearpage

    \chapter*{Conclusions}
\label{chap:concl}
\addcontentsline{toc}{chapter}{\nameref{chap:concl}} %manually adding the unnumbered chapter to toc

\hl{conclusions}

    \clearpage

    \bibliography{biblio}
\end{document}
